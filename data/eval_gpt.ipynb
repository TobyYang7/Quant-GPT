{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 19:40:36,656 - modelscope - INFO - PyTorch version 2.2.1 Found.\n",
      "2024-05-16 19:40:36,765 - modelscope - INFO - Loading ast index from /home/zhangmin/.cache/modelscope/ast_indexer\n",
      "2024-05-16 19:40:36,765 - modelscope - INFO - Loading ast index from /home/zhangmin/.cache/modelscope/ast_indexer\n",
      "2024-05-16 19:40:36,811 - modelscope - INFO - Loading done! Current index file version is 1.13.3, with md5 f225825bf0706e4b6d2ef5df7f1df86c and a total number of 972 components indexed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "import random\n",
    "from retrying import retry\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from modelscope import GenerationConfig\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer, snapshot_download\n",
    "\n",
    "openai.api_base = \"https://one.aiskt.com/v1\"\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "\n",
    "class OpenAIGPT:\n",
    "    def __init__(self, model_name=\"gpt-3.5-turbo\", keys_path=None):\n",
    "        self.model_name = model_name\n",
    "        with open(keys_path, encoding=\"utf-8\", mode=\"r\") as fr:\n",
    "            self.keys = [line.strip() for line in fr if len(line.strip()) >= 4]\n",
    "\n",
    "    def __post_process(self, response):\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    @retry(wait_fixed=300, stop_max_attempt_number=50)\n",
    "    def __call__(self, message):\n",
    "        if message is None or message == \"\":\n",
    "            return False, \"Your input is empty.\"\n",
    "\n",
    "        # current_key = random.choice(self.keys)\n",
    "        current_key = self.keys[0] if len(self.keys) == 1 else random.choice(self.keys)\n",
    "        openai.api_key = current_key\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": message}],\n",
    "            temperature=0.3,\n",
    "            top_p=0.7,\n",
    "            frequency_penalty=0.6,\n",
    "            presence_penalty=0.6,\n",
    "            n=1,\n",
    "            max_tokens=10\n",
    "        )\n",
    "        return self.__post_process(response)\n",
    "\n",
    "\n",
    "igpt = OpenAIGPT(keys_path=\"../train/apikeys.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "igpt('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json('ft_data_summary_new2_test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|██████████| 1395/1395 [02:27<00:00,  9.45it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def process_row(item):\n",
    "    def predict(prompt):\n",
    "        global igpt\n",
    "        return igpt(prompt)\n",
    "\n",
    "    text_input = f'''{item['prompt']}你只需要输出一个选项即可，不需要做任何分许判断\\n{item['content']}'''\n",
    "    predicted_response = predict(text_input).replace(\" \", \"\")\n",
    "\n",
    "    expected_response = item['label']\n",
    "    result = {\n",
    "        \"prompt\": item['prompt'],\n",
    "        \"content\": item['content'],\n",
    "        \"expected\": expected_response,\n",
    "        \"predicted\": predicted_response\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_dataset(df, max_samples=32, output_file_path='test_dataset/gpt.json'):\n",
    "    df_subset = df.head(max_samples)\n",
    "    results = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "        futures = [executor.submit(process_row, row) for _, row in df_subset.iterrows()]\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing rows\"):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                results.append(result)\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "processed_data = generate_dataset(data, max_samples=1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# from modelscope import GenerationConfig\n",
    "# from modelscope import AutoModelForCausalLM, AutoTokenizer, snapshot_download\n",
    "# from tqdm import tqdm\n",
    "# import json\n",
    "\n",
    "# # from llmtuner import ChatModel\n",
    "# # model_path = \"/home/zhangmin/toby/Quant-GPT/exp_model/v3_1\"\n",
    "# # # model_path = \"/home/zhangmin/.cache/modelscope/hub/TongyiFinance/Tongyi-Finance-14B-Chat\"\n",
    "# # model = ChatModel(dict(\n",
    "# #     model_name_or_path=model_path,\n",
    "# #     template=\"qwen\",\n",
    "# #     temperature=0.3,\n",
    "# #     top_p=0.6,\n",
    "# #     cutoff_len=8192,\n",
    "# #     max_length=256,\n",
    "# #     do_sample=True\n",
    "# #     # infer_backend=\"vllm\",\n",
    "# #     # vllm_gpu_util=0.7\n",
    "# # ))\n",
    "\n",
    "\n",
    "# def predict(prompt):\n",
    "#     return igpt(prompt)\n",
    "\n",
    "\n",
    "# def evaluate_accuracy(test_data):\n",
    "#     results = []\n",
    "#     correct_predictions = 0\n",
    "#     total_predictions = 0\n",
    "#     output_file = 'test_dataset/gpt.json'\n",
    "\n",
    "#     with tqdm(total=test_data.shape[0]) as pbar:\n",
    "#         for _, item in test_data.iterrows():\n",
    "#             text_input = f'''{item['prompt']}你只需要输出一个选项即可，不需要做任何分许判断\\n{item['content']}'''\n",
    "#             predicted_response = predict(text_input).replace(\" \", \"\")\n",
    "\n",
    "#             expected_response = item['label']\n",
    "#             result = {\n",
    "#                 \"prompt\": item['prompt'],\n",
    "#                 \"content\": item['content'],\n",
    "#                 \"expected\": expected_response,\n",
    "#                 \"predicted\": predicted_response\n",
    "#             }\n",
    "#             results.append(result)\n",
    "#             total_predictions += 1\n",
    "#             if expected_response == predicted_response:\n",
    "#                 correct_predictions += 1\n",
    "\n",
    "#             accuracy = correct_predictions / total_predictions\n",
    "#             pbar.set_description(f\"{correct_predictions}/{total_predictions}, {predicted_response}, {expected_response}, {expected_response == predicted_response}\")\n",
    "#             pbar.update()\n",
    "\n",
    "#             # 实时更新JSON文件\n",
    "#             with open(output_file, 'w') as f:\n",
    "#                 json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "#     return accuracy\n",
    "\n",
    "\n",
    "# max_samples = 10\n",
    "# test_data = pd.read_json('ft_data_summary_new_test.json')\n",
    "# # test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
    "# test_data = test_data[:max_samples]\n",
    "\n",
    "# accuracy = evaluate_accuracy(test_data)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
