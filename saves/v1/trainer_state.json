{
  "best_metric": 0.6572265625,
  "best_model_checkpoint": "/root/autodl-tmp/saves/v1/checkpoint-1200",
  "epoch": 1.9992628086988573,
  "eval_steps": 200,
  "global_step": 1356,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007371913011426466,
      "grad_norm": 13.485081590096607,
      "learning_rate": 4.411764705882353e-07,
      "loss": 5.8476,
      "step": 5
    },
    {
      "epoch": 0.014743826022852931,
      "grad_norm": 11.954792280705423,
      "learning_rate": 2.647058823529412e-06,
      "loss": 5.5523,
      "step": 10
    },
    {
      "epoch": 0.022115739034279394,
      "grad_norm": 10.390237849847708,
      "learning_rate": 4.852941176470589e-06,
      "loss": 5.7494,
      "step": 15
    },
    {
      "epoch": 0.029487652045705862,
      "grad_norm": 14.511594971982813,
      "learning_rate": 7.058823529411765e-06,
      "loss": 5.4543,
      "step": 20
    },
    {
      "epoch": 0.036859565057132324,
      "grad_norm": 53.960480403185954,
      "learning_rate": 8.823529411764707e-06,
      "loss": 5.5792,
      "step": 25
    },
    {
      "epoch": 0.04423147806855879,
      "grad_norm": 14.680594494286343,
      "learning_rate": 1.1029411764705883e-05,
      "loss": 4.5222,
      "step": 30
    },
    {
      "epoch": 0.05160339107998525,
      "grad_norm": 9.496745555503773,
      "learning_rate": 1.323529411764706e-05,
      "loss": 4.0782,
      "step": 35
    },
    {
      "epoch": 0.058975304091411725,
      "grad_norm": 10.59631845352812,
      "learning_rate": 1.5441176470588234e-05,
      "loss": 3.8476,
      "step": 40
    },
    {
      "epoch": 0.06634721710283818,
      "grad_norm": 15.42053207967761,
      "learning_rate": 1.7647058823529414e-05,
      "loss": 2.9858,
      "step": 45
    },
    {
      "epoch": 0.07371913011426465,
      "grad_norm": 11.089487479838148,
      "learning_rate": 1.9411764705882355e-05,
      "loss": 2.9019,
      "step": 50
    },
    {
      "epoch": 0.08109104312569111,
      "grad_norm": 8.826230054473038,
      "learning_rate": 2.161764705882353e-05,
      "loss": 1.3685,
      "step": 55
    },
    {
      "epoch": 0.08846295613711758,
      "grad_norm": 5.808909525109723,
      "learning_rate": 2.3823529411764704e-05,
      "loss": 1.051,
      "step": 60
    },
    {
      "epoch": 0.09583486914854404,
      "grad_norm": 12.154875819307774,
      "learning_rate": 2.6029411764705883e-05,
      "loss": 1.0789,
      "step": 65
    },
    {
      "epoch": 0.1032067821599705,
      "grad_norm": 8.870638281844247,
      "learning_rate": 2.823529411764706e-05,
      "loss": 1.0635,
      "step": 70
    },
    {
      "epoch": 0.11057869517139697,
      "grad_norm": 13.478448465149157,
      "learning_rate": 2.999995538005122e-05,
      "loss": 0.925,
      "step": 75
    },
    {
      "epoch": 0.11795060818282345,
      "grad_norm": 11.864406424353833,
      "learning_rate": 2.9998393709716884e-05,
      "loss": 0.9221,
      "step": 80
    },
    {
      "epoch": 0.12532252119424991,
      "grad_norm": 6.685786407471067,
      "learning_rate": 2.9994601307395065e-05,
      "loss": 0.768,
      "step": 85
    },
    {
      "epoch": 0.13269443420567636,
      "grad_norm": 7.832598206812836,
      "learning_rate": 2.998857873713505e-05,
      "loss": 0.8648,
      "step": 90
    },
    {
      "epoch": 0.14006634721710284,
      "grad_norm": 5.84897741096401,
      "learning_rate": 2.9980326894682094e-05,
      "loss": 0.846,
      "step": 95
    },
    {
      "epoch": 0.1474382602285293,
      "grad_norm": 5.682974406291037,
      "learning_rate": 2.9969847007344227e-05,
      "loss": 0.7193,
      "step": 100
    },
    {
      "epoch": 0.15481017323995577,
      "grad_norm": 6.082280641758144,
      "learning_rate": 2.9957140633809683e-05,
      "loss": 0.8715,
      "step": 105
    },
    {
      "epoch": 0.16218208625138222,
      "grad_norm": 8.31831231210097,
      "learning_rate": 2.99422096639151e-05,
      "loss": 0.7506,
      "step": 110
    },
    {
      "epoch": 0.1695539992628087,
      "grad_norm": 8.703527034647099,
      "learning_rate": 2.9925056318364427e-05,
      "loss": 0.8397,
      "step": 115
    },
    {
      "epoch": 0.17692591227423515,
      "grad_norm": 8.565424448214483,
      "learning_rate": 2.9905683148398642e-05,
      "loss": 1.25,
      "step": 120
    },
    {
      "epoch": 0.18429782528566163,
      "grad_norm": 6.91093155467464,
      "learning_rate": 2.9884093035416292e-05,
      "loss": 0.7536,
      "step": 125
    },
    {
      "epoch": 0.19166973829708808,
      "grad_norm": 7.900713590606254,
      "learning_rate": 2.986028919054496e-05,
      "loss": 0.6986,
      "step": 130
    },
    {
      "epoch": 0.19904165130851456,
      "grad_norm": 5.895100038071181,
      "learning_rate": 2.9834275154163654e-05,
      "loss": 0.7426,
      "step": 135
    },
    {
      "epoch": 0.206413564319941,
      "grad_norm": 4.503190387590239,
      "learning_rate": 2.9806054795376226e-05,
      "loss": 0.5955,
      "step": 140
    },
    {
      "epoch": 0.2137854773313675,
      "grad_norm": 3.572804941282203,
      "learning_rate": 2.9775632311435957e-05,
      "loss": 0.7253,
      "step": 145
    },
    {
      "epoch": 0.22115739034279394,
      "grad_norm": 6.520702315899178,
      "learning_rate": 2.9743012227121255e-05,
      "loss": 0.7034,
      "step": 150
    },
    {
      "epoch": 0.22852930335422042,
      "grad_norm": 7.206443269913305,
      "learning_rate": 2.9708199394062694e-05,
      "loss": 0.751,
      "step": 155
    },
    {
      "epoch": 0.2359012163656469,
      "grad_norm": 9.714350712007858,
      "learning_rate": 2.9671198990021432e-05,
      "loss": 0.7239,
      "step": 160
    },
    {
      "epoch": 0.24327312937707335,
      "grad_norm": 8.693342491019415,
      "learning_rate": 2.9632016518119094e-05,
      "loss": 0.6819,
      "step": 165
    },
    {
      "epoch": 0.25064504238849983,
      "grad_norm": 6.995583794081751,
      "learning_rate": 2.95906578060193e-05,
      "loss": 0.7638,
      "step": 170
    },
    {
      "epoch": 0.2580169553999263,
      "grad_norm": 8.040047157806994,
      "learning_rate": 2.954712900506091e-05,
      "loss": 0.8663,
      "step": 175
    },
    {
      "epoch": 0.26538886841135273,
      "grad_norm": 4.588849910664963,
      "learning_rate": 2.950143658934313e-05,
      "loss": 0.6322,
      "step": 180
    },
    {
      "epoch": 0.2727607814227792,
      "grad_norm": 4.2498719580994075,
      "learning_rate": 2.9453587354762582e-05,
      "loss": 0.6088,
      "step": 185
    },
    {
      "epoch": 0.2801326944342057,
      "grad_norm": 5.158653885199977,
      "learning_rate": 2.9403588418002582e-05,
      "loss": 0.6687,
      "step": 190
    },
    {
      "epoch": 0.28750460744563217,
      "grad_norm": 5.979412381689312,
      "learning_rate": 2.935144721547463e-05,
      "loss": 0.7476,
      "step": 195
    },
    {
      "epoch": 0.2948765204570586,
      "grad_norm": 7.202163606878348,
      "learning_rate": 2.9297171502212413e-05,
      "loss": 0.7559,
      "step": 200
    },
    {
      "epoch": 0.2948765204570586,
      "eval_loss": 0.7451171875,
      "eval_runtime": 163.661,
      "eval_samples_per_second": 1.845,
      "eval_steps_per_second": 1.845,
      "step": 200
    },
    {
      "epoch": 0.30224843346848507,
      "grad_norm": 4.795288400253105,
      "learning_rate": 2.9240769350718357e-05,
      "loss": 0.6208,
      "step": 205
    },
    {
      "epoch": 0.30962034647991155,
      "grad_norm": 6.695275584663396,
      "learning_rate": 2.918224914976302e-05,
      "loss": 0.7387,
      "step": 210
    },
    {
      "epoch": 0.316992259491338,
      "grad_norm": 4.224953853469141,
      "learning_rate": 2.9121619603137397e-05,
      "loss": 0.6957,
      "step": 215
    },
    {
      "epoch": 0.32436417250276445,
      "grad_norm": 4.815836945281589,
      "learning_rate": 2.9058889728358414e-05,
      "loss": 0.673,
      "step": 220
    },
    {
      "epoch": 0.3317360855141909,
      "grad_norm": 5.665839478255887,
      "learning_rate": 2.8994068855327717e-05,
      "loss": 0.805,
      "step": 225
    },
    {
      "epoch": 0.3391079985256174,
      "grad_norm": 3.9369606304844194,
      "learning_rate": 2.8927166624944043e-05,
      "loss": 0.7685,
      "step": 230
    },
    {
      "epoch": 0.3464799115370439,
      "grad_norm": 5.9032047478439145,
      "learning_rate": 2.8858192987669303e-05,
      "loss": 0.5567,
      "step": 235
    },
    {
      "epoch": 0.3538518245484703,
      "grad_norm": 6.311160688722771,
      "learning_rate": 2.8787158202048643e-05,
      "loss": 0.7461,
      "step": 240
    },
    {
      "epoch": 0.3612237375598968,
      "grad_norm": 4.017153937699014,
      "learning_rate": 2.8714072833184667e-05,
      "loss": 0.6774,
      "step": 245
    },
    {
      "epoch": 0.36859565057132326,
      "grad_norm": 4.6416350995025075,
      "learning_rate": 2.8638947751166083e-05,
      "loss": 0.823,
      "step": 250
    },
    {
      "epoch": 0.37596756358274974,
      "grad_norm": 4.05350571736143,
      "learning_rate": 2.8561794129450997e-05,
      "loss": 0.7108,
      "step": 255
    },
    {
      "epoch": 0.38333947659417617,
      "grad_norm": 3.204817130261119,
      "learning_rate": 2.8482623443205016e-05,
      "loss": 0.6672,
      "step": 260
    },
    {
      "epoch": 0.39071138960560264,
      "grad_norm": 3.8555751833405014,
      "learning_rate": 2.840144746759459e-05,
      "loss": 0.616,
      "step": 265
    },
    {
      "epoch": 0.3980833026170291,
      "grad_norm": 5.122870338241108,
      "learning_rate": 2.8318278276035626e-05,
      "loss": 0.7425,
      "step": 270
    },
    {
      "epoch": 0.4054552156284556,
      "grad_norm": 6.89496776781607,
      "learning_rate": 2.823312823839785e-05,
      "loss": 1.1273,
      "step": 275
    },
    {
      "epoch": 0.412827128639882,
      "grad_norm": 5.58282008671281,
      "learning_rate": 2.8146010019164956e-05,
      "loss": 0.6359,
      "step": 280
    },
    {
      "epoch": 0.4201990416513085,
      "grad_norm": 4.463372613517773,
      "learning_rate": 2.8056936575551028e-05,
      "loss": 0.7102,
      "step": 285
    },
    {
      "epoch": 0.427570954662735,
      "grad_norm": 7.2607123044924355,
      "learning_rate": 2.7965921155573417e-05,
      "loss": 0.7129,
      "step": 290
    },
    {
      "epoch": 0.43494286767416146,
      "grad_norm": 5.895839080007921,
      "learning_rate": 2.78729772960823e-05,
      "loss": 0.8118,
      "step": 295
    },
    {
      "epoch": 0.4423147806855879,
      "grad_norm": 3.9141625935866906,
      "learning_rate": 2.777811882074736e-05,
      "loss": 0.634,
      "step": 300
    },
    {
      "epoch": 0.44968669369701436,
      "grad_norm": 5.684474934747936,
      "learning_rate": 2.7681359838001767e-05,
      "loss": 0.7486,
      "step": 305
    },
    {
      "epoch": 0.45705860670844084,
      "grad_norm": 5.784990951833416,
      "learning_rate": 2.75827147389438e-05,
      "loss": 0.7167,
      "step": 310
    },
    {
      "epoch": 0.4644305197198673,
      "grad_norm": 7.346873091752508,
      "learning_rate": 2.748219819519643e-05,
      "loss": 0.7261,
      "step": 315
    },
    {
      "epoch": 0.4718024327312938,
      "grad_norm": 13.27724608102749,
      "learning_rate": 2.737982515672523e-05,
      "loss": 0.9497,
      "step": 320
    },
    {
      "epoch": 0.4791743457427202,
      "grad_norm": 3.022504783890868,
      "learning_rate": 2.72756108496148e-05,
      "loss": 0.6363,
      "step": 325
    },
    {
      "epoch": 0.4865462587541467,
      "grad_norm": 2.9317168862739256,
      "learning_rate": 2.716957077380419e-05,
      "loss": 0.7077,
      "step": 330
    },
    {
      "epoch": 0.4939181717655732,
      "grad_norm": 4.629183370910327,
      "learning_rate": 2.706172070078159e-05,
      "loss": 1.3452,
      "step": 335
    },
    {
      "epoch": 0.5012900847769997,
      "grad_norm": 5.591225369599257,
      "learning_rate": 2.6952076671238567e-05,
      "loss": 0.7645,
      "step": 340
    },
    {
      "epoch": 0.5086619977884261,
      "grad_norm": 4.2970982799694415,
      "learning_rate": 2.6840654992684372e-05,
      "loss": 0.6238,
      "step": 345
    },
    {
      "epoch": 0.5160339107998526,
      "grad_norm": 3.8695688933561714,
      "learning_rate": 2.672747223702045e-05,
      "loss": 0.6233,
      "step": 350
    },
    {
      "epoch": 0.523405823811279,
      "grad_norm": 5.345778532857746,
      "learning_rate": 2.6612545238075692e-05,
      "loss": 1.421,
      "step": 355
    },
    {
      "epoch": 0.5307777368227055,
      "grad_norm": 5.231957541533796,
      "learning_rate": 2.649589108910274e-05,
      "loss": 0.6434,
      "step": 360
    },
    {
      "epoch": 0.5381496498341319,
      "grad_norm": 3.9126721875561534,
      "learning_rate": 2.6377527140235656e-05,
      "loss": 0.6792,
      "step": 365
    },
    {
      "epoch": 0.5455215628455584,
      "grad_norm": 4.824414561069567,
      "learning_rate": 2.625747099590942e-05,
      "loss": 0.6328,
      "step": 370
    },
    {
      "epoch": 0.5528934758569849,
      "grad_norm": 5.045224194795531,
      "learning_rate": 2.6135740512241603e-05,
      "loss": 0.6595,
      "step": 375
    },
    {
      "epoch": 0.5602653888684114,
      "grad_norm": 3.0056104527609233,
      "learning_rate": 2.6012353794376595e-05,
      "loss": 0.6997,
      "step": 380
    },
    {
      "epoch": 0.5676373018798379,
      "grad_norm": 4.528596750673494,
      "learning_rate": 2.588732919379281e-05,
      "loss": 0.6706,
      "step": 385
    },
    {
      "epoch": 0.5750092148912643,
      "grad_norm": 4.741640421561318,
      "learning_rate": 2.576068530557324e-05,
      "loss": 0.6844,
      "step": 390
    },
    {
      "epoch": 0.5823811279026907,
      "grad_norm": 2.555146452845819,
      "learning_rate": 2.5632440965639777e-05,
      "loss": 0.6856,
      "step": 395
    },
    {
      "epoch": 0.5897530409141172,
      "grad_norm": 4.132249109693767,
      "learning_rate": 2.5502615247951727e-05,
      "loss": 0.7838,
      "step": 400
    },
    {
      "epoch": 0.5897530409141172,
      "eval_loss": 0.72021484375,
      "eval_runtime": 161.4031,
      "eval_samples_per_second": 1.871,
      "eval_steps_per_second": 1.871,
      "step": 400
    },
    {
      "epoch": 0.5971249539255437,
      "grad_norm": 5.471779634179293,
      "learning_rate": 2.5371227461668917e-05,
      "loss": 0.6281,
      "step": 405
    },
    {
      "epoch": 0.6044968669369701,
      "grad_norm": 4.960522525519492,
      "learning_rate": 2.5238297148279813e-05,
      "loss": 0.6821,
      "step": 410
    },
    {
      "epoch": 0.6118687799483966,
      "grad_norm": 4.481456998329395,
      "learning_rate": 2.5103844078695072e-05,
      "loss": 0.7017,
      "step": 415
    },
    {
      "epoch": 0.6192406929598231,
      "grad_norm": 4.379747124493394,
      "learning_rate": 2.496788825030702e-05,
      "loss": 0.6843,
      "step": 420
    },
    {
      "epoch": 0.6266126059712496,
      "grad_norm": 2.602481647609054,
      "learning_rate": 2.4830449884015377e-05,
      "loss": 0.6876,
      "step": 425
    },
    {
      "epoch": 0.633984518982676,
      "grad_norm": 5.438737024348754,
      "learning_rate": 2.46915494212198e-05,
      "loss": 0.648,
      "step": 430
    },
    {
      "epoch": 0.6413564319941024,
      "grad_norm": 2.324943705463684,
      "learning_rate": 2.45512075207796e-05,
      "loss": 0.6426,
      "step": 435
    },
    {
      "epoch": 0.6487283450055289,
      "grad_norm": 5.664500091478611,
      "learning_rate": 2.44094450559411e-05,
      "loss": 0.744,
      "step": 440
    },
    {
      "epoch": 0.6561002580169554,
      "grad_norm": 3.5162675108259562,
      "learning_rate": 2.426628311123315e-05,
      "loss": 0.6438,
      "step": 445
    },
    {
      "epoch": 0.6634721710283819,
      "grad_norm": 3.8926774332452827,
      "learning_rate": 2.412174297933121e-05,
      "loss": 0.6961,
      "step": 450
    },
    {
      "epoch": 0.6708440840398083,
      "grad_norm": 5.708113014762936,
      "learning_rate": 2.3975846157890415e-05,
      "loss": 1.649,
      "step": 455
    },
    {
      "epoch": 0.6782159970512348,
      "grad_norm": 6.145541129352976,
      "learning_rate": 2.382861434634824e-05,
      "loss": 0.6666,
      "step": 460
    },
    {
      "epoch": 0.6855879100626613,
      "grad_norm": 3.2624992826415515,
      "learning_rate": 2.368006944269709e-05,
      "loss": 0.6827,
      "step": 465
    },
    {
      "epoch": 0.6929598230740878,
      "grad_norm": 4.006693993617764,
      "learning_rate": 2.3530233540227382e-05,
      "loss": 0.5792,
      "step": 470
    },
    {
      "epoch": 0.7003317360855142,
      "grad_norm": 3.7236367771759444,
      "learning_rate": 2.3379128924241614e-05,
      "loss": 0.7623,
      "step": 475
    },
    {
      "epoch": 0.7077036490969406,
      "grad_norm": 4.042195994517884,
      "learning_rate": 2.3226778068739782e-05,
      "loss": 0.7549,
      "step": 480
    },
    {
      "epoch": 0.7150755621083671,
      "grad_norm": 3.7506845391034127,
      "learning_rate": 2.307320363307683e-05,
      "loss": 0.6626,
      "step": 485
    },
    {
      "epoch": 0.7224474751197936,
      "grad_norm": 5.183719039487194,
      "learning_rate": 2.2918428458592488e-05,
      "loss": 0.6452,
      "step": 490
    },
    {
      "epoch": 0.72981938813122,
      "grad_norm": 2.721971303438292,
      "learning_rate": 2.276247556521404e-05,
      "loss": 0.6126,
      "step": 495
    },
    {
      "epoch": 0.7371913011426465,
      "grad_norm": 5.896620595175357,
      "learning_rate": 2.2605368148032548e-05,
      "loss": 0.7026,
      "step": 500
    },
    {
      "epoch": 0.744563214154073,
      "grad_norm": 6.154308073329744,
      "learning_rate": 2.2447129573852997e-05,
      "loss": 0.8481,
      "step": 505
    },
    {
      "epoch": 0.7519351271654995,
      "grad_norm": 2.3360530756524063,
      "learning_rate": 2.228778337771893e-05,
      "loss": 1.1989,
      "step": 510
    },
    {
      "epoch": 0.759307040176926,
      "grad_norm": 3.190227384221378,
      "learning_rate": 2.2127353259412055e-05,
      "loss": 0.6396,
      "step": 515
    },
    {
      "epoch": 0.7666789531883523,
      "grad_norm": 2.9175786814185916,
      "learning_rate": 2.1965863079927328e-05,
      "loss": 0.6964,
      "step": 520
    },
    {
      "epoch": 0.7740508661997788,
      "grad_norm": 4.610086892461901,
      "learning_rate": 2.1803336857924073e-05,
      "loss": 0.6712,
      "step": 525
    },
    {
      "epoch": 0.7814227792112053,
      "grad_norm": 7.004427732321611,
      "learning_rate": 2.1639798766153678e-05,
      "loss": 0.692,
      "step": 530
    },
    {
      "epoch": 0.7887946922226318,
      "grad_norm": 2.9829867411672075,
      "learning_rate": 2.1475273127864323e-05,
      "loss": 0.6678,
      "step": 535
    },
    {
      "epoch": 0.7961666052340582,
      "grad_norm": 2.923647227205263,
      "learning_rate": 2.1309784413183346e-05,
      "loss": 0.5784,
      "step": 540
    },
    {
      "epoch": 0.8035385182454847,
      "grad_norm": 5.092171614705864,
      "learning_rate": 2.1143357235477796e-05,
      "loss": 0.818,
      "step": 545
    },
    {
      "epoch": 0.8109104312569112,
      "grad_norm": 2.3187805920476707,
      "learning_rate": 2.0976016347693626e-05,
      "loss": 0.7636,
      "step": 550
    },
    {
      "epoch": 0.8182823442683377,
      "grad_norm": 2.5254419117991196,
      "learning_rate": 2.0807786638674154e-05,
      "loss": 0.7178,
      "step": 555
    },
    {
      "epoch": 0.825654257279764,
      "grad_norm": 2.5750993721418314,
      "learning_rate": 2.063869312945834e-05,
      "loss": 0.6789,
      "step": 560
    },
    {
      "epoch": 0.8330261702911905,
      "grad_norm": 3.793320942857495,
      "learning_rate": 2.0468760969559333e-05,
      "loss": 0.6801,
      "step": 565
    },
    {
      "epoch": 0.840398083302617,
      "grad_norm": 3.3734410567183213,
      "learning_rate": 2.0298015433223968e-05,
      "loss": 0.6908,
      "step": 570
    },
    {
      "epoch": 0.8477699963140435,
      "grad_norm": 2.102640579267003,
      "learning_rate": 2.0126481915673694e-05,
      "loss": 0.6437,
      "step": 575
    },
    {
      "epoch": 0.85514190932547,
      "grad_norm": 2.096367104242699,
      "learning_rate": 1.995418592932751e-05,
      "loss": 0.6277,
      "step": 580
    },
    {
      "epoch": 0.8625138223368964,
      "grad_norm": 2.083682803978623,
      "learning_rate": 1.978115310000744e-05,
      "loss": 0.6466,
      "step": 585
    },
    {
      "epoch": 0.8698857353483229,
      "grad_norm": 4.068216888620227,
      "learning_rate": 1.960740916312721e-05,
      "loss": 0.683,
      "step": 590
    },
    {
      "epoch": 0.8772576483597494,
      "grad_norm": 2.7631366737650644,
      "learning_rate": 1.9432979959864564e-05,
      "loss": 0.6466,
      "step": 595
    },
    {
      "epoch": 0.8846295613711758,
      "grad_norm": 2.5662247287485225,
      "learning_rate": 1.925789143331785e-05,
      "loss": 0.9792,
      "step": 600
    },
    {
      "epoch": 0.8846295613711758,
      "eval_loss": 0.70654296875,
      "eval_runtime": 161.3441,
      "eval_samples_per_second": 1.872,
      "eval_steps_per_second": 1.872,
      "step": 600
    },
    {
      "epoch": 0.8920014743826022,
      "grad_norm": 5.019261614121691,
      "learning_rate": 1.9082169624647513e-05,
      "loss": 0.6781,
      "step": 605
    },
    {
      "epoch": 0.8993733873940287,
      "grad_norm": 5.348788981530768,
      "learning_rate": 1.8905840669202904e-05,
      "loss": 0.6747,
      "step": 610
    },
    {
      "epoch": 0.9067453004054552,
      "grad_norm": 5.082746121740716,
      "learning_rate": 1.8728930792635185e-05,
      "loss": 0.6524,
      "step": 615
    },
    {
      "epoch": 0.9141172134168817,
      "grad_norm": 1.9134436873712761,
      "learning_rate": 1.85514663069967e-05,
      "loss": 0.6488,
      "step": 620
    },
    {
      "epoch": 0.9214891264283082,
      "grad_norm": 3.4840807994593748,
      "learning_rate": 1.83734736068276e-05,
      "loss": 0.6269,
      "step": 625
    },
    {
      "epoch": 0.9288610394397346,
      "grad_norm": 2.4739365362753842,
      "learning_rate": 1.8194979165230094e-05,
      "loss": 0.6362,
      "step": 630
    },
    {
      "epoch": 0.9362329524511611,
      "grad_norm": 2.855859585025198,
      "learning_rate": 1.8016009529931103e-05,
      "loss": 0.6348,
      "step": 635
    },
    {
      "epoch": 0.9436048654625876,
      "grad_norm": 5.151142562799623,
      "learning_rate": 1.783659131933377e-05,
      "loss": 0.6774,
      "step": 640
    },
    {
      "epoch": 0.950976778474014,
      "grad_norm": 3.172591422510923,
      "learning_rate": 1.7656751218558467e-05,
      "loss": 0.765,
      "step": 645
    },
    {
      "epoch": 0.9583486914854404,
      "grad_norm": 3.229189114378738,
      "learning_rate": 1.7476515975473885e-05,
      "loss": 0.7231,
      "step": 650
    },
    {
      "epoch": 0.9657206044968669,
      "grad_norm": 3.039680690265996,
      "learning_rate": 1.7295912396718768e-05,
      "loss": 0.6105,
      "step": 655
    },
    {
      "epoch": 0.9730925175082934,
      "grad_norm": 2.4362962374997177,
      "learning_rate": 1.7114967343714946e-05,
      "loss": 0.6354,
      "step": 660
    },
    {
      "epoch": 0.9804644305197199,
      "grad_norm": 5.646396397342471,
      "learning_rate": 1.6933707728672184e-05,
      "loss": 0.6542,
      "step": 665
    },
    {
      "epoch": 0.9878363435311464,
      "grad_norm": 2.8543823828100208,
      "learning_rate": 1.6752160510585507e-05,
      "loss": 0.6398,
      "step": 670
    },
    {
      "epoch": 0.9952082565425728,
      "grad_norm": 3.8003929449672675,
      "learning_rate": 1.6570352691225535e-05,
      "loss": 0.6583,
      "step": 675
    },
    {
      "epoch": 1.0025801695539993,
      "grad_norm": 4.127096671387708,
      "learning_rate": 1.6388311311122473e-05,
      "loss": 0.7048,
      "step": 680
    },
    {
      "epoch": 1.0099520825654258,
      "grad_norm": 3.880901222070666,
      "learning_rate": 1.6206063445544347e-05,
      "loss": 0.6036,
      "step": 685
    },
    {
      "epoch": 1.0173239955768523,
      "grad_norm": 4.020764600836119,
      "learning_rate": 1.6023636200470068e-05,
      "loss": 0.65,
      "step": 690
    },
    {
      "epoch": 1.0246959085882787,
      "grad_norm": 6.222484229868888,
      "learning_rate": 1.5841056708557876e-05,
      "loss": 0.6159,
      "step": 695
    },
    {
      "epoch": 1.0320678215997052,
      "grad_norm": 5.0990812106540435,
      "learning_rate": 1.5658352125109932e-05,
      "loss": 0.5253,
      "step": 700
    },
    {
      "epoch": 1.0394397346111315,
      "grad_norm": 3.513847814844108,
      "learning_rate": 1.547554962403341e-05,
      "loss": 0.6453,
      "step": 705
    },
    {
      "epoch": 1.046811647622558,
      "grad_norm": 3.8971383249241693,
      "learning_rate": 1.529267639379892e-05,
      "loss": 0.7148,
      "step": 710
    },
    {
      "epoch": 1.0541835606339844,
      "grad_norm": 3.384733327090027,
      "learning_rate": 1.5109759633396713e-05,
      "loss": 0.6709,
      "step": 715
    },
    {
      "epoch": 1.061555473645411,
      "grad_norm": 4.430094914708578,
      "learning_rate": 1.492682654829135e-05,
      "loss": 0.6362,
      "step": 720
    },
    {
      "epoch": 1.0689273866568374,
      "grad_norm": 10.638364502959679,
      "learning_rate": 1.474390434637538e-05,
      "loss": 0.8591,
      "step": 725
    },
    {
      "epoch": 1.0762992996682639,
      "grad_norm": 4.9412858315985,
      "learning_rate": 1.4561020233922692e-05,
      "loss": 0.6139,
      "step": 730
    },
    {
      "epoch": 1.0836712126796904,
      "grad_norm": 3.4499502936931568,
      "learning_rate": 1.4378201411542052e-05,
      "loss": 0.5663,
      "step": 735
    },
    {
      "epoch": 1.0910431256911168,
      "grad_norm": 3.9540821152721346,
      "learning_rate": 1.4195475070131575e-05,
      "loss": 0.6831,
      "step": 740
    },
    {
      "epoch": 1.0984150387025433,
      "grad_norm": 6.397233441811505,
      "learning_rate": 1.401286838683452e-05,
      "loss": 0.6251,
      "step": 745
    },
    {
      "epoch": 1.1057869517139698,
      "grad_norm": 3.4751843422184687,
      "learning_rate": 1.3830408520997253e-05,
      "loss": 0.6269,
      "step": 750
    },
    {
      "epoch": 1.1131588647253963,
      "grad_norm": 3.480884557665462,
      "learning_rate": 1.3648122610129773e-05,
      "loss": 0.6353,
      "step": 755
    },
    {
      "epoch": 1.1205307777368227,
      "grad_norm": 3.228372739521277,
      "learning_rate": 1.3466037765869487e-05,
      "loss": 0.8423,
      "step": 760
    },
    {
      "epoch": 1.1279026907482492,
      "grad_norm": 2.485079394834359,
      "learning_rate": 1.3284181069948925e-05,
      "loss": 0.6893,
      "step": 765
    },
    {
      "epoch": 1.1352746037596757,
      "grad_norm": 2.976768386976556,
      "learning_rate": 1.310257957016776e-05,
      "loss": 0.6209,
      "step": 770
    },
    {
      "epoch": 1.1426465167711022,
      "grad_norm": 2.556016477195734,
      "learning_rate": 1.2921260276370015e-05,
      "loss": 0.6158,
      "step": 775
    },
    {
      "epoch": 1.1500184297825287,
      "grad_norm": 2.68624407861221,
      "learning_rate": 1.274025015642681e-05,
      "loss": 0.6381,
      "step": 780
    },
    {
      "epoch": 1.157390342793955,
      "grad_norm": 3.398619417268379,
      "learning_rate": 1.2559576132225411e-05,
      "loss": 0.6068,
      "step": 785
    },
    {
      "epoch": 1.1647622558053814,
      "grad_norm": 2.671440752384085,
      "learning_rate": 1.2379265075665097e-05,
      "loss": 0.6492,
      "step": 790
    },
    {
      "epoch": 1.1721341688168079,
      "grad_norm": 3.498958072993419,
      "learning_rate": 1.2199343804660457e-05,
      "loss": 0.7599,
      "step": 795
    },
    {
      "epoch": 1.1795060818282344,
      "grad_norm": 1.5332689946997566,
      "learning_rate": 1.2019839079152747e-05,
      "loss": 0.5938,
      "step": 800
    },
    {
      "epoch": 1.1795060818282344,
      "eval_loss": 0.67333984375,
      "eval_runtime": 162.2962,
      "eval_samples_per_second": 1.861,
      "eval_steps_per_second": 1.861,
      "step": 800
    },
    {
      "epoch": 1.1868779948396608,
      "grad_norm": 4.947505098381654,
      "learning_rate": 1.1840777597129809e-05,
      "loss": 0.6623,
      "step": 805
    },
    {
      "epoch": 1.1942499078510873,
      "grad_norm": 3.069626265300902,
      "learning_rate": 1.1662185990655285e-05,
      "loss": 0.6295,
      "step": 810
    },
    {
      "epoch": 1.2016218208625138,
      "grad_norm": 2.9889023758919175,
      "learning_rate": 1.1484090821907563e-05,
      "loss": 0.5698,
      "step": 815
    },
    {
      "epoch": 1.2089937338739403,
      "grad_norm": 3.4218583828312945,
      "learning_rate": 1.1306518579229153e-05,
      "loss": 0.6292,
      "step": 820
    },
    {
      "epoch": 1.2163656468853667,
      "grad_norm": 8.124176111694549,
      "learning_rate": 1.1129495673187048e-05,
      "loss": 0.7099,
      "step": 825
    },
    {
      "epoch": 1.2237375598967932,
      "grad_norm": 5.210950483640713,
      "learning_rate": 1.0953048432644634e-05,
      "loss": 0.658,
      "step": 830
    },
    {
      "epoch": 1.2311094729082197,
      "grad_norm": 2.554726504480639,
      "learning_rate": 1.0777203100845775e-05,
      "loss": 0.5851,
      "step": 835
    },
    {
      "epoch": 1.2384813859196462,
      "grad_norm": 3.3170327729056512,
      "learning_rate": 1.0601985831511595e-05,
      "loss": 0.6127,
      "step": 840
    },
    {
      "epoch": 1.2458532989310727,
      "grad_norm": 2.8391776080624704,
      "learning_rate": 1.0427422684950621e-05,
      "loss": 0.6114,
      "step": 845
    },
    {
      "epoch": 1.2532252119424991,
      "grad_norm": 2.9580474473872496,
      "learning_rate": 1.0253539624182788e-05,
      "loss": 0.6399,
      "step": 850
    },
    {
      "epoch": 1.2605971249539256,
      "grad_norm": 3.7201550367633325,
      "learning_rate": 1.0080362511077922e-05,
      "loss": 0.6324,
      "step": 855
    },
    {
      "epoch": 1.267969037965352,
      "grad_norm": 4.390238133305269,
      "learning_rate": 9.907917102509273e-06,
      "loss": 0.5559,
      "step": 860
    },
    {
      "epoch": 1.2753409509767786,
      "grad_norm": 6.24794431880902,
      "learning_rate": 9.73622904652265e-06,
      "loss": 0.6591,
      "step": 865
    },
    {
      "epoch": 1.282712863988205,
      "grad_norm": 9.620504602176618,
      "learning_rate": 9.56532387852176e-06,
      "loss": 0.6176,
      "step": 870
    },
    {
      "epoch": 1.2900847769996315,
      "grad_norm": 6.308703863837472,
      "learning_rate": 9.395227017470304e-06,
      "loss": 0.6665,
      "step": 875
    },
    {
      "epoch": 1.2974566900110578,
      "grad_norm": 5.7834047001458035,
      "learning_rate": 9.225963762111336e-06,
      "loss": 0.6458,
      "step": 880
    },
    {
      "epoch": 1.3048286030224843,
      "grad_norm": 2.521110486825889,
      "learning_rate": 9.0575592872046e-06,
      "loss": 0.6455,
      "step": 885
    },
    {
      "epoch": 1.3122005160339107,
      "grad_norm": 4.085649801922364,
      "learning_rate": 8.89003863978221e-06,
      "loss": 0.6962,
      "step": 890
    },
    {
      "epoch": 1.3195724290453372,
      "grad_norm": 3.370309297560614,
      "learning_rate": 8.723426735423398e-06,
      "loss": 0.6528,
      "step": 895
    },
    {
      "epoch": 1.3269443420567637,
      "grad_norm": 3.3056630148375827,
      "learning_rate": 8.557748354548773e-06,
      "loss": 0.6175,
      "step": 900
    },
    {
      "epoch": 1.3343162550681902,
      "grad_norm": 2.4218404818686565,
      "learning_rate": 8.393028138734685e-06,
      "loss": 0.6126,
      "step": 905
    },
    {
      "epoch": 1.3416881680796167,
      "grad_norm": 2.3435388706810985,
      "learning_rate": 8.22929058704828e-06,
      "loss": 0.6304,
      "step": 910
    },
    {
      "epoch": 1.3490600810910431,
      "grad_norm": 3.071105065689009,
      "learning_rate": 8.066560052403674e-06,
      "loss": 0.6304,
      "step": 915
    },
    {
      "epoch": 1.3564319941024696,
      "grad_norm": 3.882607093418103,
      "learning_rate": 7.904860737939959e-06,
      "loss": 0.5616,
      "step": 920
    },
    {
      "epoch": 1.363803907113896,
      "grad_norm": 2.5001758783990655,
      "learning_rate": 7.744216693421403e-06,
      "loss": 0.6403,
      "step": 925
    },
    {
      "epoch": 1.3711758201253226,
      "grad_norm": 2.648843334277868,
      "learning_rate": 7.584651811660504e-06,
      "loss": 0.6643,
      "step": 930
    },
    {
      "epoch": 1.378547733136749,
      "grad_norm": 3.075346336219891,
      "learning_rate": 7.426189824964384e-06,
      "loss": 0.6281,
      "step": 935
    },
    {
      "epoch": 1.3859196461481755,
      "grad_norm": 4.016409137285621,
      "learning_rate": 7.268854301605037e-06,
      "loss": 0.8428,
      "step": 940
    },
    {
      "epoch": 1.3932915591596018,
      "grad_norm": 8.591092380291302,
      "learning_rate": 7.112668642314013e-06,
      "loss": 0.6991,
      "step": 945
    },
    {
      "epoch": 1.4006634721710283,
      "grad_norm": 3.63565251792148,
      "learning_rate": 6.957656076801956e-06,
      "loss": 0.7412,
      "step": 950
    },
    {
      "epoch": 1.4080353851824547,
      "grad_norm": 4.401654442533995,
      "learning_rate": 6.80383966030366e-06,
      "loss": 0.6314,
      "step": 955
    },
    {
      "epoch": 1.4154072981938812,
      "grad_norm": 5.891030591510821,
      "learning_rate": 6.651242270148988e-06,
      "loss": 0.6528,
      "step": 960
    },
    {
      "epoch": 1.4227792112053077,
      "grad_norm": 4.311851801874521,
      "learning_rate": 6.4998866023603334e-06,
      "loss": 0.603,
      "step": 965
    },
    {
      "epoch": 1.4301511242167342,
      "grad_norm": 6.988667227730257,
      "learning_rate": 6.349795168276994e-06,
      "loss": 0.6008,
      "step": 970
    },
    {
      "epoch": 1.4375230372281607,
      "grad_norm": 3.640589195449455,
      "learning_rate": 6.200990291207011e-06,
      "loss": 0.6383,
      "step": 975
    },
    {
      "epoch": 1.4448949502395871,
      "grad_norm": 3.6412469055199943,
      "learning_rate": 6.053494103107028e-06,
      "loss": 0.628,
      "step": 980
    },
    {
      "epoch": 1.4522668632510136,
      "grad_norm": 2.7762680640792192,
      "learning_rate": 5.907328541290535e-06,
      "loss": 0.6372,
      "step": 985
    },
    {
      "epoch": 1.45963877626244,
      "grad_norm": 3.070483585264714,
      "learning_rate": 5.7625153451651456e-06,
      "loss": 0.5634,
      "step": 990
    },
    {
      "epoch": 1.4670106892738666,
      "grad_norm": 4.106943097376365,
      "learning_rate": 5.619076052999216e-06,
      "loss": 0.6154,
      "step": 995
    },
    {
      "epoch": 1.474382602285293,
      "grad_norm": 3.7980826512392647,
      "learning_rate": 5.47703199871847e-06,
      "loss": 0.602,
      "step": 1000
    },
    {
      "epoch": 1.474382602285293,
      "eval_loss": 0.67529296875,
      "eval_runtime": 161.688,
      "eval_samples_per_second": 1.868,
      "eval_steps_per_second": 1.868,
      "step": 1000
    },
    {
      "epoch": 1.4817545152967195,
      "grad_norm": 3.6092080844783787,
      "learning_rate": 5.3364043087329565e-06,
      "loss": 0.5767,
      "step": 1005
    },
    {
      "epoch": 1.489126428308146,
      "grad_norm": 3.174772905159261,
      "learning_rate": 5.197213898794875e-06,
      "loss": 0.6289,
      "step": 1010
    },
    {
      "epoch": 1.4964983413195725,
      "grad_norm": 5.518393956950107,
      "learning_rate": 5.0594814708877815e-06,
      "loss": 0.6682,
      "step": 1015
    },
    {
      "epoch": 1.503870254330999,
      "grad_norm": 4.239659553437408,
      "learning_rate": 4.923227510147516e-06,
      "loss": 0.5652,
      "step": 1020
    },
    {
      "epoch": 1.5112421673424254,
      "grad_norm": 4.1793459370813215,
      "learning_rate": 4.788472281815453e-06,
      "loss": 0.5792,
      "step": 1025
    },
    {
      "epoch": 1.518614080353852,
      "grad_norm": 6.461254920401069,
      "learning_rate": 4.6552358282243924e-06,
      "loss": 0.6435,
      "step": 1030
    },
    {
      "epoch": 1.5259859933652784,
      "grad_norm": 2.545625977012075,
      "learning_rate": 4.5235379658176675e-06,
      "loss": 0.6156,
      "step": 1035
    },
    {
      "epoch": 1.5333579063767049,
      "grad_norm": 4.9052866024651065,
      "learning_rate": 4.393398282201788e-06,
      "loss": 0.6335,
      "step": 1040
    },
    {
      "epoch": 1.5407298193881314,
      "grad_norm": 2.3237162065986103,
      "learning_rate": 4.2648361332331694e-06,
      "loss": 0.5983,
      "step": 1045
    },
    {
      "epoch": 1.5481017323995578,
      "grad_norm": 3.1003836175723194,
      "learning_rate": 4.137870640139307e-06,
      "loss": 0.564,
      "step": 1050
    },
    {
      "epoch": 1.555473645410984,
      "grad_norm": 3.1734988989993766,
      "learning_rate": 4.012520686674826e-06,
      "loss": 0.5543,
      "step": 1055
    },
    {
      "epoch": 1.5628455584224106,
      "grad_norm": 2.9885376168363154,
      "learning_rate": 3.888804916312912e-06,
      "loss": 0.5865,
      "step": 1060
    },
    {
      "epoch": 1.570217471433837,
      "grad_norm": 4.7954845253811005,
      "learning_rate": 3.7667417294724055e-06,
      "loss": 0.6305,
      "step": 1065
    },
    {
      "epoch": 1.5775893844452635,
      "grad_norm": 5.305787363565094,
      "learning_rate": 3.646349280781111e-06,
      "loss": 0.6079,
      "step": 1070
    },
    {
      "epoch": 1.58496129745669,
      "grad_norm": 3.1229659864195547,
      "learning_rate": 3.5276454763756105e-06,
      "loss": 0.6142,
      "step": 1075
    },
    {
      "epoch": 1.5923332104681165,
      "grad_norm": 3.0504279469248896,
      "learning_rate": 3.41064797123808e-06,
      "loss": 0.8261,
      "step": 1080
    },
    {
      "epoch": 1.599705123479543,
      "grad_norm": 4.962211391198164,
      "learning_rate": 3.2953741665704156e-06,
      "loss": 0.64,
      "step": 1085
    },
    {
      "epoch": 1.6070770364909694,
      "grad_norm": 2.0951960330214465,
      "learning_rate": 3.181841207206152e-06,
      "loss": 0.6183,
      "step": 1090
    },
    {
      "epoch": 1.614448949502396,
      "grad_norm": 4.726506408104441,
      "learning_rate": 3.070065979060477e-06,
      "loss": 0.5882,
      "step": 1095
    },
    {
      "epoch": 1.6218208625138222,
      "grad_norm": 3.146088473486461,
      "learning_rate": 2.960065106618757e-06,
      "loss": 0.5834,
      "step": 1100
    },
    {
      "epoch": 1.6291927755252487,
      "grad_norm": 2.9863202618839155,
      "learning_rate": 2.8518549504639733e-06,
      "loss": 0.5863,
      "step": 1105
    },
    {
      "epoch": 1.6365646885366751,
      "grad_norm": 5.939412785213266,
      "learning_rate": 2.7454516048433704e-06,
      "loss": 0.6411,
      "step": 1110
    },
    {
      "epoch": 1.6439366015481016,
      "grad_norm": 3.252831128095922,
      "learning_rate": 2.6408708952747567e-06,
      "loss": 0.5822,
      "step": 1115
    },
    {
      "epoch": 1.651308514559528,
      "grad_norm": 4.346240560141879,
      "learning_rate": 2.5381283761927266e-06,
      "loss": 0.579,
      "step": 1120
    },
    {
      "epoch": 1.6586804275709546,
      "grad_norm": 3.6351735701017343,
      "learning_rate": 2.437239328635258e-06,
      "loss": 0.6168,
      "step": 1125
    },
    {
      "epoch": 1.666052340582381,
      "grad_norm": 3.329046828792352,
      "learning_rate": 2.33821875797092e-06,
      "loss": 0.6028,
      "step": 1130
    },
    {
      "epoch": 1.6734242535938075,
      "grad_norm": 3.9017298203236357,
      "learning_rate": 2.241081391667111e-06,
      "loss": 0.6625,
      "step": 1135
    },
    {
      "epoch": 1.680796166605234,
      "grad_norm": 3.7657845915172885,
      "learning_rate": 2.1458416770996235e-06,
      "loss": 0.5687,
      "step": 1140
    },
    {
      "epoch": 1.6881680796166605,
      "grad_norm": 3.762374175256628,
      "learning_rate": 2.05251377940386e-06,
      "loss": 0.6386,
      "step": 1145
    },
    {
      "epoch": 1.695539992628087,
      "grad_norm": 2.593191036589723,
      "learning_rate": 1.9611115793680438e-06,
      "loss": 0.6536,
      "step": 1150
    },
    {
      "epoch": 1.7029119056395134,
      "grad_norm": 4.985849102637292,
      "learning_rate": 1.8716486713686949e-06,
      "loss": 0.6463,
      "step": 1155
    },
    {
      "epoch": 1.71028381865094,
      "grad_norm": 4.220868871126981,
      "learning_rate": 1.7841383613487367e-06,
      "loss": 0.6697,
      "step": 1160
    },
    {
      "epoch": 1.7176557316623664,
      "grad_norm": 2.5193267321220123,
      "learning_rate": 1.6985936648384648e-06,
      "loss": 0.5741,
      "step": 1165
    },
    {
      "epoch": 1.7250276446737929,
      "grad_norm": 4.350664770873343,
      "learning_rate": 1.6150273050197474e-06,
      "loss": 0.6002,
      "step": 1170
    },
    {
      "epoch": 1.7323995576852194,
      "grad_norm": 3.984589037558626,
      "learning_rate": 1.5334517108336805e-06,
      "loss": 0.6049,
      "step": 1175
    },
    {
      "epoch": 1.7397714706966458,
      "grad_norm": 3.843133112069749,
      "learning_rate": 1.4538790151320092e-06,
      "loss": 0.8995,
      "step": 1180
    },
    {
      "epoch": 1.7471433837080723,
      "grad_norm": 3.896317546380188,
      "learning_rate": 1.3763210528726055e-06,
      "loss": 0.5968,
      "step": 1185
    },
    {
      "epoch": 1.7545152967194988,
      "grad_norm": 3.211618717898759,
      "learning_rate": 1.3007893593592203e-06,
      "loss": 0.6096,
      "step": 1190
    },
    {
      "epoch": 1.7618872097309253,
      "grad_norm": 3.816590395900997,
      "learning_rate": 1.227295168525841e-06,
      "loss": 0.6361,
      "step": 1195
    },
    {
      "epoch": 1.7692591227423518,
      "grad_norm": 4.519515867857812,
      "learning_rate": 1.1558494112658351e-06,
      "loss": 0.5666,
      "step": 1200
    },
    {
      "epoch": 1.7692591227423518,
      "eval_loss": 0.6572265625,
      "eval_runtime": 164.1305,
      "eval_samples_per_second": 1.84,
      "eval_steps_per_second": 1.84,
      "step": 1200
    },
    {
      "epoch": 1.7766310357537782,
      "grad_norm": 3.3705197814598415,
      "learning_rate": 1.0864627138062011e-06,
      "loss": 0.6546,
      "step": 1205
    },
    {
      "epoch": 1.7840029487652047,
      "grad_norm": 5.53410498005197,
      "learning_rate": 1.0191453961270942e-06,
      "loss": 0.6304,
      "step": 1210
    },
    {
      "epoch": 1.7913748617766312,
      "grad_norm": 4.273342774020228,
      "learning_rate": 9.539074704269396e-07,
      "loss": 0.5653,
      "step": 1215
    },
    {
      "epoch": 1.7987467747880577,
      "grad_norm": 3.120732705082358,
      "learning_rate": 8.907586396333006e-07,
      "loss": 0.5926,
      "step": 1220
    },
    {
      "epoch": 1.806118687799484,
      "grad_norm": 4.224316966646425,
      "learning_rate": 8.297082959597324e-07,
      "loss": 0.6149,
      "step": 1225
    },
    {
      "epoch": 1.8134906008109104,
      "grad_norm": 4.133244122463186,
      "learning_rate": 7.707655195088914e-07,
      "loss": 0.6314,
      "step": 1230
    },
    {
      "epoch": 1.8208625138223369,
      "grad_norm": 4.09877908092692,
      "learning_rate": 7.139390769220155e-07,
      "loss": 0.6514,
      "step": 1235
    },
    {
      "epoch": 1.8282344268337634,
      "grad_norm": 3.5078245183952586,
      "learning_rate": 6.592374200750672e-07,
      "loss": 0.6061,
      "step": 1240
    },
    {
      "epoch": 1.8356063398451898,
      "grad_norm": 3.2558791919418466,
      "learning_rate": 6.066686848216607e-07,
      "loss": 0.7156,
      "step": 1245
    },
    {
      "epoch": 1.8429782528566163,
      "grad_norm": 5.182621095312299,
      "learning_rate": 5.56240689783013e-07,
      "loss": 0.5672,
      "step": 1250
    },
    {
      "epoch": 1.8503501658680428,
      "grad_norm": 2.502616759349501,
      "learning_rate": 5.079609351850628e-07,
      "loss": 0.6367,
      "step": 1255
    },
    {
      "epoch": 1.8577220788794693,
      "grad_norm": 4.680508525022893,
      "learning_rate": 4.6183660174296427e-07,
      "loss": 0.6473,
      "step": 1260
    },
    {
      "epoch": 1.8650939918908955,
      "grad_norm": 3.4902416044958153,
      "learning_rate": 4.178745495930764e-07,
      "loss": 1.2011,
      "step": 1265
    },
    {
      "epoch": 1.872465904902322,
      "grad_norm": 3.4817922803723516,
      "learning_rate": 3.760813172726457e-07,
      "loss": 0.6095,
      "step": 1270
    },
    {
      "epoch": 1.8798378179137485,
      "grad_norm": 3.9653754231993603,
      "learning_rate": 3.3646312074733285e-07,
      "loss": 0.5781,
      "step": 1275
    },
    {
      "epoch": 1.887209730925175,
      "grad_norm": 4.2605018181539425,
      "learning_rate": 2.990258524866851e-07,
      "loss": 0.6379,
      "step": 1280
    },
    {
      "epoch": 1.8945816439366014,
      "grad_norm": 4.320813445667081,
      "learning_rate": 2.6377508058776256e-07,
      "loss": 0.6208,
      "step": 1285
    },
    {
      "epoch": 1.901953556948028,
      "grad_norm": 2.412124801812217,
      "learning_rate": 2.3071604794696555e-07,
      "loss": 0.5569,
      "step": 1290
    },
    {
      "epoch": 1.9093254699594544,
      "grad_norm": 2.662295084523452,
      "learning_rate": 1.9985367148026846e-07,
      "loss": 0.5654,
      "step": 1295
    },
    {
      "epoch": 1.9166973829708809,
      "grad_norm": 2.790288493752411,
      "learning_rate": 1.7119254139190964e-07,
      "loss": 0.5542,
      "step": 1300
    },
    {
      "epoch": 1.9240692959823074,
      "grad_norm": 2.293384348543161,
      "learning_rate": 1.447369204916893e-07,
      "loss": 0.5967,
      "step": 1305
    },
    {
      "epoch": 1.9314412089937338,
      "grad_norm": 3.221830278333804,
      "learning_rate": 1.204907435609548e-07,
      "loss": 0.5674,
      "step": 1310
    },
    {
      "epoch": 1.9388131220051603,
      "grad_norm": 4.661356022569648,
      "learning_rate": 9.845761676737231e-08,
      "loss": 0.5597,
      "step": 1315
    },
    {
      "epoch": 1.9461850350165868,
      "grad_norm": 3.4921414910865285,
      "learning_rate": 7.86408171285774e-08,
      "loss": 0.5807,
      "step": 1320
    },
    {
      "epoch": 1.9535569480280133,
      "grad_norm": 2.925461566240372,
      "learning_rate": 6.104329202478665e-08,
      "loss": 0.5968,
      "step": 1325
    },
    {
      "epoch": 1.9609288610394398,
      "grad_norm": 3.655833678777428,
      "learning_rate": 4.566765876041656e-08,
      "loss": 0.6193,
      "step": 1330
    },
    {
      "epoch": 1.9683007740508662,
      "grad_norm": 4.741395457554593,
      "learning_rate": 3.251620417482171e-08,
      "loss": 0.6137,
      "step": 1335
    },
    {
      "epoch": 1.9756726870622927,
      "grad_norm": 3.0998699774299574,
      "learning_rate": 2.1590884302158388e-08,
      "loss": 0.5193,
      "step": 1340
    },
    {
      "epoch": 1.9830446000737192,
      "grad_norm": 2.7262210327301006,
      "learning_rate": 1.289332408047239e-08,
      "loss": 0.6165,
      "step": 1345
    },
    {
      "epoch": 1.9904165130851457,
      "grad_norm": 3.7831462179451476,
      "learning_rate": 6.424817110005643e-09,
      "loss": 0.5826,
      "step": 1350
    },
    {
      "epoch": 1.9977884260965721,
      "grad_norm": 3.029576921704021,
      "learning_rate": 2.1863254608150974e-09,
      "loss": 0.6568,
      "step": 1355
    },
    {
      "epoch": 1.9992628086988573,
      "step": 1356,
      "total_flos": 931691237146624.0,
      "train_loss": 0.8344805894699772,
      "train_runtime": 10334.0354,
      "train_samples_per_second": 0.525,
      "train_steps_per_second": 0.131
    }
  ],
  "logging_steps": 5,
  "max_steps": 1356,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 200,
  "total_flos": 931691237146624.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
