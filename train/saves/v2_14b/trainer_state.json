{
  "best_metric": 0.60498046875,
  "best_model_checkpoint": "/root/autodl-tmp/saves/v2/checkpoint-1200",
  "epoch": 2.9948246118458886,
  "eval_steps": 200,
  "global_step": 1302,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.011500862564692352,
      "grad_norm": 6.456799297700482,
      "learning_rate": 9.090909090909091e-07,
      "loss": 4.0185,
      "step": 5
    },
    {
      "epoch": 0.023001725129384705,
      "grad_norm": 8.910616927364472,
      "learning_rate": 2.7272727272727272e-06,
      "loss": 3.6063,
      "step": 10
    },
    {
      "epoch": 0.034502587694077054,
      "grad_norm": 7.090830824804395,
      "learning_rate": 4.5454545454545455e-06,
      "loss": 3.6912,
      "step": 15
    },
    {
      "epoch": 0.04600345025876941,
      "grad_norm": 6.535055796898255,
      "learning_rate": 6.818181818181818e-06,
      "loss": 2.8594,
      "step": 20
    },
    {
      "epoch": 0.05750431282346176,
      "grad_norm": 6.698431424798722,
      "learning_rate": 8.636363636363637e-06,
      "loss": 3.1351,
      "step": 25
    },
    {
      "epoch": 0.06900517538815411,
      "grad_norm": 10.594701411859146,
      "learning_rate": 1.0909090909090909e-05,
      "loss": 3.0823,
      "step": 30
    },
    {
      "epoch": 0.08050603795284646,
      "grad_norm": 7.35856227015718,
      "learning_rate": 1.3181818181818183e-05,
      "loss": 2.694,
      "step": 35
    },
    {
      "epoch": 0.09200690051753882,
      "grad_norm": 8.006700395923767,
      "learning_rate": 1.5e-05,
      "loss": 1.8079,
      "step": 40
    },
    {
      "epoch": 0.10350776308223117,
      "grad_norm": 10.77711170738624,
      "learning_rate": 1.7272727272727274e-05,
      "loss": 2.0099,
      "step": 45
    },
    {
      "epoch": 0.11500862564692352,
      "grad_norm": 6.4404650062155655,
      "learning_rate": 1.9545454545454546e-05,
      "loss": 1.5761,
      "step": 50
    },
    {
      "epoch": 0.12650948821161587,
      "grad_norm": 10.007195206092577,
      "learning_rate": 2.1818181818181818e-05,
      "loss": 0.8646,
      "step": 55
    },
    {
      "epoch": 0.13801035077630822,
      "grad_norm": 8.214169960388205,
      "learning_rate": 2.409090909090909e-05,
      "loss": 1.2938,
      "step": 60
    },
    {
      "epoch": 0.14951121334100057,
      "grad_norm": 6.758251879452972,
      "learning_rate": 2.6363636363636365e-05,
      "loss": 1.0234,
      "step": 65
    },
    {
      "epoch": 0.16101207590569291,
      "grad_norm": 6.81403642769354,
      "learning_rate": 2.8636363636363637e-05,
      "loss": 0.874,
      "step": 70
    },
    {
      "epoch": 0.1725129384703853,
      "grad_norm": 3.8954781892838724,
      "learning_rate": 2.9999806186902096e-05,
      "loss": 0.917,
      "step": 75
    },
    {
      "epoch": 0.18401380103507764,
      "grad_norm": 5.090943693902109,
      "learning_rate": 2.999762584706943e-05,
      "loss": 0.9306,
      "step": 80
    },
    {
      "epoch": 0.19551466359977,
      "grad_norm": 4.265790122321366,
      "learning_rate": 2.9993023254350274e-05,
      "loss": 0.6751,
      "step": 85
    },
    {
      "epoch": 0.20701552616446234,
      "grad_norm": 7.0005876205040485,
      "learning_rate": 2.9985999152105187e-05,
      "loss": 0.8193,
      "step": 90
    },
    {
      "epoch": 0.2185163887291547,
      "grad_norm": 3.3326572470772944,
      "learning_rate": 2.9976554674790533e-05,
      "loss": 0.6611,
      "step": 95
    },
    {
      "epoch": 0.23001725129384704,
      "grad_norm": 3.264262356071991,
      "learning_rate": 2.9964691347775225e-05,
      "loss": 0.6049,
      "step": 100
    },
    {
      "epoch": 0.24151811385853938,
      "grad_norm": 6.651082156886279,
      "learning_rate": 2.9950411087094405e-05,
      "loss": 0.7309,
      "step": 105
    },
    {
      "epoch": 0.25301897642323173,
      "grad_norm": 3.4225545798832044,
      "learning_rate": 2.9933716199139968e-05,
      "loss": 0.66,
      "step": 110
    },
    {
      "epoch": 0.2645198389879241,
      "grad_norm": 5.618132807263203,
      "learning_rate": 2.991460938028806e-05,
      "loss": 0.6953,
      "step": 115
    },
    {
      "epoch": 0.27602070155261643,
      "grad_norm": 4.254874298961308,
      "learning_rate": 2.9893093716463575e-05,
      "loss": 0.6302,
      "step": 120
    },
    {
      "epoch": 0.2875215641173088,
      "grad_norm": 3.4854883056172765,
      "learning_rate": 2.9869172682641788e-05,
      "loss": 0.7056,
      "step": 125
    },
    {
      "epoch": 0.29902242668200113,
      "grad_norm": 4.351692453986446,
      "learning_rate": 2.9842850142287066e-05,
      "loss": 0.7359,
      "step": 130
    },
    {
      "epoch": 0.3105232892466935,
      "grad_norm": 2.5092978555816616,
      "learning_rate": 2.9814130346728924e-05,
      "loss": 0.6902,
      "step": 135
    },
    {
      "epoch": 0.32202415181138583,
      "grad_norm": 3.439208633195761,
      "learning_rate": 2.9783017934475383e-05,
      "loss": 0.647,
      "step": 140
    },
    {
      "epoch": 0.33352501437607823,
      "grad_norm": 3.146366122207581,
      "learning_rate": 2.9749517930463785e-05,
      "loss": 0.6094,
      "step": 145
    },
    {
      "epoch": 0.3450258769407706,
      "grad_norm": 2.7521245024944725,
      "learning_rate": 2.971363574524927e-05,
      "loss": 0.6251,
      "step": 150
    },
    {
      "epoch": 0.35652673950546293,
      "grad_norm": 1.849533098262221,
      "learning_rate": 2.967537717413088e-05,
      "loss": 0.5977,
      "step": 155
    },
    {
      "epoch": 0.3680276020701553,
      "grad_norm": 3.2650436046942195,
      "learning_rate": 2.9634748396215572e-05,
      "loss": 0.6187,
      "step": 160
    },
    {
      "epoch": 0.37952846463484763,
      "grad_norm": 3.746263549748773,
      "learning_rate": 2.9591755973420255e-05,
      "loss": 0.7185,
      "step": 165
    },
    {
      "epoch": 0.39102932719954,
      "grad_norm": 2.6301356447461566,
      "learning_rate": 2.9546406849411963e-05,
      "loss": 0.6523,
      "step": 170
    },
    {
      "epoch": 0.4025301897642323,
      "grad_norm": 3.654099567127287,
      "learning_rate": 2.94987083484864e-05,
      "loss": 0.7818,
      "step": 175
    },
    {
      "epoch": 0.4140310523289247,
      "grad_norm": 2.664546219140019,
      "learning_rate": 2.9448668174384996e-05,
      "loss": 0.6734,
      "step": 180
    },
    {
      "epoch": 0.425531914893617,
      "grad_norm": 11.930237316861145,
      "learning_rate": 2.939629440905067e-05,
      "loss": 0.7015,
      "step": 185
    },
    {
      "epoch": 0.4370327774583094,
      "grad_norm": 6.806024435088946,
      "learning_rate": 2.9341595511322555e-05,
      "loss": 0.7468,
      "step": 190
    },
    {
      "epoch": 0.4485336400230017,
      "grad_norm": 2.645892265494515,
      "learning_rate": 2.9284580315569784e-05,
      "loss": 0.7244,
      "step": 195
    },
    {
      "epoch": 0.46003450258769407,
      "grad_norm": 2.7991161578190202,
      "learning_rate": 2.9225258030264675e-05,
      "loss": 0.6791,
      "step": 200
    },
    {
      "epoch": 0.46003450258769407,
      "eval_loss": 0.6357421875,
      "eval_runtime": 361.2314,
      "eval_samples_per_second": 1.071,
      "eval_steps_per_second": 0.537,
      "step": 200
    },
    {
      "epoch": 0.4715353651523864,
      "grad_norm": 2.606401138221003,
      "learning_rate": 2.916363823649548e-05,
      "loss": 0.6782,
      "step": 205
    },
    {
      "epoch": 0.48303622771707877,
      "grad_norm": 4.663759318937226,
      "learning_rate": 2.9099730886418953e-05,
      "loss": 0.7014,
      "step": 210
    },
    {
      "epoch": 0.4945370902817711,
      "grad_norm": 3.3622914777682773,
      "learning_rate": 2.9033546301652978e-05,
      "loss": 0.7253,
      "step": 215
    },
    {
      "epoch": 0.5060379528464635,
      "grad_norm": 3.6231000199254364,
      "learning_rate": 2.8965095171609544e-05,
      "loss": 0.6717,
      "step": 220
    },
    {
      "epoch": 0.5175388154111559,
      "grad_norm": 3.2755975858213517,
      "learning_rate": 2.8894388551768297e-05,
      "loss": 0.6204,
      "step": 225
    },
    {
      "epoch": 0.5290396779758482,
      "grad_norm": 2.1415549753751346,
      "learning_rate": 2.882143786189099e-05,
      "loss": 0.6193,
      "step": 230
    },
    {
      "epoch": 0.5405405405405406,
      "grad_norm": 2.3196683401981337,
      "learning_rate": 2.8746254884177074e-05,
      "loss": 0.681,
      "step": 235
    },
    {
      "epoch": 0.5520414031052329,
      "grad_norm": 3.3069472473722903,
      "learning_rate": 2.866885176136079e-05,
      "loss": 0.7448,
      "step": 240
    },
    {
      "epoch": 0.5635422656699253,
      "grad_norm": 1.9038693310358519,
      "learning_rate": 2.8589240994749977e-05,
      "loss": 0.6263,
      "step": 245
    },
    {
      "epoch": 0.5750431282346176,
      "grad_norm": 2.0861848879474745,
      "learning_rate": 2.850743544220703e-05,
      "loss": 0.5879,
      "step": 250
    },
    {
      "epoch": 0.58654399079931,
      "grad_norm": 3.045634780462428,
      "learning_rate": 2.8423448316072195e-05,
      "loss": 0.6864,
      "step": 255
    },
    {
      "epoch": 0.5980448533640023,
      "grad_norm": 3.525300277652768,
      "learning_rate": 2.83372931810297e-05,
      "loss": 0.6731,
      "step": 260
    },
    {
      "epoch": 0.6095457159286947,
      "grad_norm": 2.7161919430525208,
      "learning_rate": 2.824898395191691e-05,
      "loss": 0.6434,
      "step": 265
    },
    {
      "epoch": 0.621046578493387,
      "grad_norm": 3.302084065236546,
      "learning_rate": 2.8158534891476967e-05,
      "loss": 0.5951,
      "step": 270
    },
    {
      "epoch": 0.6325474410580794,
      "grad_norm": 2.68161539580823,
      "learning_rate": 2.8065960608055214e-05,
      "loss": 0.6841,
      "step": 275
    },
    {
      "epoch": 0.6440483036227717,
      "grad_norm": 3.0532370144429266,
      "learning_rate": 2.7971276053239817e-05,
      "loss": 0.6197,
      "step": 280
    },
    {
      "epoch": 0.6555491661874641,
      "grad_norm": 4.434495727765817,
      "learning_rate": 2.7874496519446968e-05,
      "loss": 0.6043,
      "step": 285
    },
    {
      "epoch": 0.6670500287521565,
      "grad_norm": 3.5052158246193565,
      "learning_rate": 2.7775637637450992e-05,
      "loss": 0.662,
      "step": 290
    },
    {
      "epoch": 0.6785508913168488,
      "grad_norm": 3.9156054875808306,
      "learning_rate": 2.767471537385985e-05,
      "loss": 0.6195,
      "step": 295
    },
    {
      "epoch": 0.6900517538815412,
      "grad_norm": 2.2025427334249637,
      "learning_rate": 2.757174602853639e-05,
      "loss": 0.6538,
      "step": 300
    },
    {
      "epoch": 0.7015526164462335,
      "grad_norm": 2.079480431466224,
      "learning_rate": 2.7466746231965754e-05,
      "loss": 0.6526,
      "step": 305
    },
    {
      "epoch": 0.7130534790109259,
      "grad_norm": 2.4871308011706557,
      "learning_rate": 2.735973294256943e-05,
      "loss": 0.5424,
      "step": 310
    },
    {
      "epoch": 0.7245543415756182,
      "grad_norm": 2.2459731546682553,
      "learning_rate": 2.725072344396629e-05,
      "loss": 0.7093,
      "step": 315
    },
    {
      "epoch": 0.7360552041403106,
      "grad_norm": 2.8266585348156674,
      "learning_rate": 2.7139735342181134e-05,
      "loss": 0.5723,
      "step": 320
    },
    {
      "epoch": 0.7475560667050029,
      "grad_norm": 2.086484997099198,
      "learning_rate": 2.702678656280116e-05,
      "loss": 0.6019,
      "step": 325
    },
    {
      "epoch": 0.7590569292696953,
      "grad_norm": 1.9933665580552715,
      "learning_rate": 2.6911895348080822e-05,
      "loss": 0.658,
      "step": 330
    },
    {
      "epoch": 0.7705577918343876,
      "grad_norm": 2.4891004948070776,
      "learning_rate": 2.6795080253995544e-05,
      "loss": 0.6188,
      "step": 335
    },
    {
      "epoch": 0.78205865439908,
      "grad_norm": 2.982051237906251,
      "learning_rate": 2.6676360147244726e-05,
      "loss": 0.7691,
      "step": 340
    },
    {
      "epoch": 0.7935595169637722,
      "grad_norm": 8.973780576215661,
      "learning_rate": 2.6555754202204655e-05,
      "loss": 0.5597,
      "step": 345
    },
    {
      "epoch": 0.8050603795284647,
      "grad_norm": 3.452813612392761,
      "learning_rate": 2.643328189783164e-05,
      "loss": 0.6468,
      "step": 350
    },
    {
      "epoch": 0.816561242093157,
      "grad_norm": 3.8222896555278516,
      "learning_rate": 2.630896301451596e-05,
      "loss": 0.658,
      "step": 355
    },
    {
      "epoch": 0.8280621046578494,
      "grad_norm": 2.2579111861050487,
      "learning_rate": 2.6182817630887208e-05,
      "loss": 0.5591,
      "step": 360
    },
    {
      "epoch": 0.8395629672225416,
      "grad_norm": 2.6815355591270533,
      "learning_rate": 2.605486612057136e-05,
      "loss": 0.6659,
      "step": 365
    },
    {
      "epoch": 0.851063829787234,
      "grad_norm": 1.7709426675389228,
      "learning_rate": 2.5925129148900274e-05,
      "loss": 0.6701,
      "step": 370
    },
    {
      "epoch": 0.8625646923519263,
      "grad_norm": 3.018562666235379,
      "learning_rate": 2.579362766957404e-05,
      "loss": 0.6613,
      "step": 375
    },
    {
      "epoch": 0.8740655549166187,
      "grad_norm": 2.5671699509698764,
      "learning_rate": 2.5660382921276784e-05,
      "loss": 0.6214,
      "step": 380
    },
    {
      "epoch": 0.8855664174813112,
      "grad_norm": 5.004859449183581,
      "learning_rate": 2.55254164242464e-05,
      "loss": 0.5928,
      "step": 385
    },
    {
      "epoch": 0.8970672800460034,
      "grad_norm": 2.175836475005652,
      "learning_rate": 2.5388749976798856e-05,
      "loss": 0.5747,
      "step": 390
    },
    {
      "epoch": 0.9085681426106959,
      "grad_norm": 2.662760995695471,
      "learning_rate": 2.525040565180757e-05,
      "loss": 0.6396,
      "step": 395
    },
    {
      "epoch": 0.9200690051753881,
      "grad_norm": 2.96401165931427,
      "learning_rate": 2.5110405793138426e-05,
      "loss": 0.6368,
      "step": 400
    },
    {
      "epoch": 0.9200690051753881,
      "eval_loss": 0.6240234375,
      "eval_runtime": 361.0472,
      "eval_samples_per_second": 1.072,
      "eval_steps_per_second": 0.537,
      "step": 400
    },
    {
      "epoch": 0.9315698677400805,
      "grad_norm": 2.291354459709655,
      "learning_rate": 2.4968773012041048e-05,
      "loss": 0.5963,
      "step": 405
    },
    {
      "epoch": 0.9430707303047728,
      "grad_norm": 2.5325718482300403,
      "learning_rate": 2.4825530183496864e-05,
      "loss": 0.5931,
      "step": 410
    },
    {
      "epoch": 0.9545715928694652,
      "grad_norm": 4.207484824177328,
      "learning_rate": 2.4680700442524614e-05,
      "loss": 0.6577,
      "step": 415
    },
    {
      "epoch": 0.9660724554341575,
      "grad_norm": 2.171013305610917,
      "learning_rate": 2.453430718044381e-05,
      "loss": 0.633,
      "step": 420
    },
    {
      "epoch": 0.9775733179988499,
      "grad_norm": 2.9568543493982817,
      "learning_rate": 2.4386374041096836e-05,
      "loss": 0.6362,
      "step": 425
    },
    {
      "epoch": 0.9890741805635422,
      "grad_norm": 1.8063140609466006,
      "learning_rate": 2.423692491703024e-05,
      "loss": 0.6986,
      "step": 430
    },
    {
      "epoch": 1.0005750431282345,
      "grad_norm": 4.2407786927632865,
      "learning_rate": 2.40859839456359e-05,
      "loss": 0.65,
      "step": 435
    },
    {
      "epoch": 1.012075905692927,
      "grad_norm": 1.849127680001087,
      "learning_rate": 2.3933575505252584e-05,
      "loss": 0.5997,
      "step": 440
    },
    {
      "epoch": 1.0235767682576193,
      "grad_norm": 2.8818491916126487,
      "learning_rate": 2.3779724211228632e-05,
      "loss": 0.6482,
      "step": 445
    },
    {
      "epoch": 1.0350776308223117,
      "grad_norm": 4.592850890289913,
      "learning_rate": 2.362445491194638e-05,
      "loss": 0.6116,
      "step": 450
    },
    {
      "epoch": 1.046578493387004,
      "grad_norm": 2.17254602245074,
      "learning_rate": 2.3467792684808893e-05,
      "loss": 0.5999,
      "step": 455
    },
    {
      "epoch": 1.0580793559516963,
      "grad_norm": 3.964390438553889,
      "learning_rate": 2.3309762832189733e-05,
      "loss": 0.7141,
      "step": 460
    },
    {
      "epoch": 1.0695802185163887,
      "grad_norm": 1.5470548045226853,
      "learning_rate": 2.315039087734643e-05,
      "loss": 0.5944,
      "step": 465
    },
    {
      "epoch": 1.0810810810810811,
      "grad_norm": 3.008762506640853,
      "learning_rate": 2.2989702560298207e-05,
      "loss": 0.5903,
      "step": 470
    },
    {
      "epoch": 1.0925819436457735,
      "grad_norm": 2.937273451315902,
      "learning_rate": 2.2827723833668747e-05,
      "loss": 0.6205,
      "step": 475
    },
    {
      "epoch": 1.1040828062104657,
      "grad_norm": 3.3449921162957157,
      "learning_rate": 2.2664480858494616e-05,
      "loss": 0.6254,
      "step": 480
    },
    {
      "epoch": 1.1155836687751581,
      "grad_norm": 1.8256263984559542,
      "learning_rate": 2.25e-05,
      "loss": 0.7085,
      "step": 485
    },
    {
      "epoch": 1.1270845313398505,
      "grad_norm": 2.010009220185911,
      "learning_rate": 2.2334307823338505e-05,
      "loss": 0.541,
      "step": 490
    },
    {
      "epoch": 1.1385853939045427,
      "grad_norm": 2.50882996700975,
      "learning_rate": 2.2167431089302625e-05,
      "loss": 0.6034,
      "step": 495
    },
    {
      "epoch": 1.1500862564692351,
      "grad_norm": 2.276209681509586,
      "learning_rate": 2.1999396750001627e-05,
      "loss": 0.5926,
      "step": 500
    },
    {
      "epoch": 1.1615871190339275,
      "grad_norm": 1.6372581507952084,
      "learning_rate": 2.183023194450857e-05,
      "loss": 0.6368,
      "step": 505
    },
    {
      "epoch": 1.17308798159862,
      "grad_norm": 2.5434420777649254,
      "learning_rate": 2.1659963994477055e-05,
      "loss": 0.5664,
      "step": 510
    },
    {
      "epoch": 1.1845888441633123,
      "grad_norm": 3.002286236980055,
      "learning_rate": 2.148862039972857e-05,
      "loss": 0.5774,
      "step": 515
    },
    {
      "epoch": 1.1960897067280045,
      "grad_norm": 1.9991424331495067,
      "learning_rate": 2.1316228833810994e-05,
      "loss": 0.5802,
      "step": 520
    },
    {
      "epoch": 1.207590569292697,
      "grad_norm": 2.352802721891456,
      "learning_rate": 2.1142817139529078e-05,
      "loss": 0.5862,
      "step": 525
    },
    {
      "epoch": 1.2190914318573893,
      "grad_norm": 1.5542135499229037,
      "learning_rate": 2.0968413324447564e-05,
      "loss": 0.5819,
      "step": 530
    },
    {
      "epoch": 1.2305922944220817,
      "grad_norm": 2.8558313290818194,
      "learning_rate": 2.0793045556367742e-05,
      "loss": 0.5232,
      "step": 535
    },
    {
      "epoch": 1.242093156986774,
      "grad_norm": 1.830452823266102,
      "learning_rate": 2.0616742158778047e-05,
      "loss": 0.5657,
      "step": 540
    },
    {
      "epoch": 1.2535940195514663,
      "grad_norm": 2.085672368498893,
      "learning_rate": 2.043953160627961e-05,
      "loss": 0.5723,
      "step": 545
    },
    {
      "epoch": 1.2650948821161587,
      "grad_norm": 2.5750641554091867,
      "learning_rate": 2.02614425199873e-05,
      "loss": 0.6178,
      "step": 550
    },
    {
      "epoch": 1.2765957446808511,
      "grad_norm": 1.8081872330973048,
      "learning_rate": 2.00825036629072e-05,
      "loss": 0.5838,
      "step": 555
    },
    {
      "epoch": 1.2880966072455435,
      "grad_norm": 2.9386990088598695,
      "learning_rate": 1.9902743935291076e-05,
      "loss": 0.6175,
      "step": 560
    },
    {
      "epoch": 1.2995974698102357,
      "grad_norm": 1.883885399224151,
      "learning_rate": 1.9722192369968776e-05,
      "loss": 0.6135,
      "step": 565
    },
    {
      "epoch": 1.3110983323749281,
      "grad_norm": 2.2857567387946003,
      "learning_rate": 1.9540878127659105e-05,
      "loss": 0.6674,
      "step": 570
    },
    {
      "epoch": 1.3225991949396205,
      "grad_norm": 3.8259042926813693,
      "learning_rate": 1.935883049226015e-05,
      "loss": 0.6653,
      "step": 575
    },
    {
      "epoch": 1.3341000575043127,
      "grad_norm": 2.8192966268070445,
      "learning_rate": 1.9176078866119646e-05,
      "loss": 0.5636,
      "step": 580
    },
    {
      "epoch": 1.3456009200690051,
      "grad_norm": 2.436188273821835,
      "learning_rate": 1.8992652765286225e-05,
      "loss": 0.6507,
      "step": 585
    },
    {
      "epoch": 1.3571017826336975,
      "grad_norm": 1.9103639144696227,
      "learning_rate": 1.8808581814742323e-05,
      "loss": 0.5856,
      "step": 590
    },
    {
      "epoch": 1.36860264519839,
      "grad_norm": 2.999861153537162,
      "learning_rate": 1.8623895743619453e-05,
      "loss": 0.6042,
      "step": 595
    },
    {
      "epoch": 1.3801035077630823,
      "grad_norm": 1.8128489450110372,
      "learning_rate": 1.84386243803967e-05,
      "loss": 0.5558,
      "step": 600
    },
    {
      "epoch": 1.3801035077630823,
      "eval_loss": 0.6259765625,
      "eval_runtime": 361.1543,
      "eval_samples_per_second": 1.072,
      "eval_steps_per_second": 0.537,
      "step": 600
    },
    {
      "epoch": 1.3916043703277745,
      "grad_norm": 3.5050458725742515,
      "learning_rate": 1.8252797648083134e-05,
      "loss": 0.5021,
      "step": 605
    },
    {
      "epoch": 1.403105232892467,
      "grad_norm": 2.548847432715697,
      "learning_rate": 1.8066445559384997e-05,
      "loss": 0.5657,
      "step": 610
    },
    {
      "epoch": 1.4146060954571593,
      "grad_norm": 3.1761557018853193,
      "learning_rate": 1.7879598211858344e-05,
      "loss": 0.6185,
      "step": 615
    },
    {
      "epoch": 1.4261069580218515,
      "grad_norm": 2.496910686094855,
      "learning_rate": 1.7692285783048054e-05,
      "loss": 0.6069,
      "step": 620
    },
    {
      "epoch": 1.437607820586544,
      "grad_norm": 1.9571632984267926,
      "learning_rate": 1.7504538525613833e-05,
      "loss": 0.5679,
      "step": 625
    },
    {
      "epoch": 1.4491086831512363,
      "grad_norm": 2.9892692202693807,
      "learning_rate": 1.7316386762444182e-05,
      "loss": 0.5757,
      "step": 630
    },
    {
      "epoch": 1.4606095457159287,
      "grad_norm": 3.8350298571525876,
      "learning_rate": 1.7127860881758953e-05,
      "loss": 0.6254,
      "step": 635
    },
    {
      "epoch": 1.4721104082806211,
      "grad_norm": 2.6838738568734275,
      "learning_rate": 1.6938991332201362e-05,
      "loss": 0.5906,
      "step": 640
    },
    {
      "epoch": 1.4836112708453135,
      "grad_norm": 3.253802787221562,
      "learning_rate": 1.6749808617920288e-05,
      "loss": 0.5984,
      "step": 645
    },
    {
      "epoch": 1.4951121334100057,
      "grad_norm": 2.234549244629407,
      "learning_rate": 1.6560343293643554e-05,
      "loss": 0.5975,
      "step": 650
    },
    {
      "epoch": 1.506612995974698,
      "grad_norm": 2.842698606215153,
      "learning_rate": 1.6370625959743065e-05,
      "loss": 0.6251,
      "step": 655
    },
    {
      "epoch": 1.5181138585393903,
      "grad_norm": 1.9001032559785789,
      "learning_rate": 1.6180687257292577e-05,
      "loss": 0.6031,
      "step": 660
    },
    {
      "epoch": 1.5296147211040827,
      "grad_norm": 2.978935597678221,
      "learning_rate": 1.5990557863118863e-05,
      "loss": 0.6349,
      "step": 665
    },
    {
      "epoch": 1.541115583668775,
      "grad_norm": 4.206185409244089,
      "learning_rate": 1.5800268484847156e-05,
      "loss": 0.5844,
      "step": 670
    },
    {
      "epoch": 1.5526164462334675,
      "grad_norm": 2.5662667360263147,
      "learning_rate": 1.560984985594157e-05,
      "loss": 0.5831,
      "step": 675
    },
    {
      "epoch": 1.56411730879816,
      "grad_norm": 3.668879381892561,
      "learning_rate": 1.5419332730741376e-05,
      "loss": 0.6034,
      "step": 680
    },
    {
      "epoch": 1.5756181713628523,
      "grad_norm": 2.7561055670929004,
      "learning_rate": 1.522874787949389e-05,
      "loss": 0.598,
      "step": 685
    },
    {
      "epoch": 1.5871190339275447,
      "grad_norm": 1.5186931491205733,
      "learning_rate": 1.5038126083384795e-05,
      "loss": 0.5835,
      "step": 690
    },
    {
      "epoch": 1.598619896492237,
      "grad_norm": 3.7513315413161163,
      "learning_rate": 1.4847498129566707e-05,
      "loss": 0.5868,
      "step": 695
    },
    {
      "epoch": 1.6101207590569293,
      "grad_norm": 3.591065789886794,
      "learning_rate": 1.465689480618677e-05,
      "loss": 0.587,
      "step": 700
    },
    {
      "epoch": 1.6216216216216215,
      "grad_norm": 2.4821159210903385,
      "learning_rate": 1.4466346897414087e-05,
      "loss": 0.6342,
      "step": 705
    },
    {
      "epoch": 1.633122484186314,
      "grad_norm": 3.5721482745595834,
      "learning_rate": 1.4275885178467805e-05,
      "loss": 0.6205,
      "step": 710
    },
    {
      "epoch": 1.6446233467510063,
      "grad_norm": 3.37937877267014,
      "learning_rate": 1.4085540410646615e-05,
      "loss": 0.5881,
      "step": 715
    },
    {
      "epoch": 1.6561242093156987,
      "grad_norm": 2.107062731186596,
      "learning_rate": 1.3895343336360556e-05,
      "loss": 0.6057,
      "step": 720
    },
    {
      "epoch": 1.667625071880391,
      "grad_norm": 4.048000931478153,
      "learning_rate": 1.3705324674165813e-05,
      "loss": 0.6423,
      "step": 725
    },
    {
      "epoch": 1.6791259344450835,
      "grad_norm": 4.597277247884101,
      "learning_rate": 1.3515515113803383e-05,
      "loss": 0.676,
      "step": 730
    },
    {
      "epoch": 1.6906267970097757,
      "grad_norm": 3.2397138487797776,
      "learning_rate": 1.3325945311242435e-05,
      "loss": 0.5962,
      "step": 735
    },
    {
      "epoch": 1.702127659574468,
      "grad_norm": 3.334548647980065,
      "learning_rate": 1.313664588372907e-05,
      "loss": 0.5508,
      "step": 740
    },
    {
      "epoch": 1.7136285221391603,
      "grad_norm": 2.19506746655672,
      "learning_rate": 1.294764740484135e-05,
      "loss": 0.628,
      "step": 745
    },
    {
      "epoch": 1.7251293847038527,
      "grad_norm": 1.9534080251421064,
      "learning_rate": 1.2758980399551409e-05,
      "loss": 0.5804,
      "step": 750
    },
    {
      "epoch": 1.736630247268545,
      "grad_norm": 3.5768573631968295,
      "learning_rate": 1.2570675339295388e-05,
      "loss": 0.5929,
      "step": 755
    },
    {
      "epoch": 1.7481311098332375,
      "grad_norm": 3.02651925562435,
      "learning_rate": 1.2382762637052005e-05,
      "loss": 0.6167,
      "step": 760
    },
    {
      "epoch": 1.75963197239793,
      "grad_norm": 2.6314246890495205,
      "learning_rate": 1.2195272642430589e-05,
      "loss": 0.5917,
      "step": 765
    },
    {
      "epoch": 1.7711328349626223,
      "grad_norm": 3.2773799679451026,
      "learning_rate": 1.2008235636769353e-05,
      "loss": 0.585,
      "step": 770
    },
    {
      "epoch": 1.7826336975273147,
      "grad_norm": 2.3711614152394365,
      "learning_rate": 1.1821681828244681e-05,
      "loss": 0.6108,
      "step": 775
    },
    {
      "epoch": 1.794134560092007,
      "grad_norm": 2.2464995385279534,
      "learning_rate": 1.1635641346992219e-05,
      "loss": 0.5882,
      "step": 780
    },
    {
      "epoch": 1.8056354226566993,
      "grad_norm": 2.642938473984709,
      "learning_rate": 1.1450144240240595e-05,
      "loss": 0.5671,
      "step": 785
    },
    {
      "epoch": 1.8171362852213915,
      "grad_norm": 2.281369332862584,
      "learning_rate": 1.1265220467458523e-05,
      "loss": 0.5954,
      "step": 790
    },
    {
      "epoch": 1.8286371477860839,
      "grad_norm": 3.343483388636692,
      "learning_rate": 1.1080899895516056e-05,
      "loss": 0.598,
      "step": 795
    },
    {
      "epoch": 1.8401380103507763,
      "grad_norm": 2.51575088775453,
      "learning_rate": 1.0897212293860836e-05,
      "loss": 0.5704,
      "step": 800
    },
    {
      "epoch": 1.8401380103507763,
      "eval_loss": 0.63330078125,
      "eval_runtime": 361.1083,
      "eval_samples_per_second": 1.072,
      "eval_steps_per_second": 0.537,
      "step": 800
    },
    {
      "epoch": 1.8516388729154687,
      "grad_norm": 3.525316309406747,
      "learning_rate": 1.0714187329710066e-05,
      "loss": 0.6166,
      "step": 805
    },
    {
      "epoch": 1.863139735480161,
      "grad_norm": 2.1252860123919564,
      "learning_rate": 1.0531854563258938e-05,
      "loss": 0.5818,
      "step": 810
    },
    {
      "epoch": 1.8746405980448535,
      "grad_norm": 1.8955331842043266,
      "learning_rate": 1.0350243442906451e-05,
      "loss": 0.5803,
      "step": 815
    },
    {
      "epoch": 1.8861414606095457,
      "grad_norm": 2.180273386866544,
      "learning_rate": 1.016938330049919e-05,
      "loss": 0.5544,
      "step": 820
    },
    {
      "epoch": 1.897642323174238,
      "grad_norm": 2.1356528810651882,
      "learning_rate": 9.989303346593974e-06,
      "loss": 0.5979,
      "step": 825
    },
    {
      "epoch": 1.9091431857389303,
      "grad_norm": 3.477130602332882,
      "learning_rate": 9.810032665740087e-06,
      "loss": 0.5951,
      "step": 830
    },
    {
      "epoch": 1.9206440483036227,
      "grad_norm": 3.323158781630515,
      "learning_rate": 9.631600211781877e-06,
      "loss": 0.5839,
      "step": 835
    },
    {
      "epoch": 1.932144910868315,
      "grad_norm": 3.9737990948942916,
      "learning_rate": 9.454034803182402e-06,
      "loss": 0.6036,
      "step": 840
    },
    {
      "epoch": 1.9436457734330075,
      "grad_norm": 3.6414911548882083,
      "learning_rate": 9.27736511836903e-06,
      "loss": 0.666,
      "step": 845
    },
    {
      "epoch": 1.9551466359976999,
      "grad_norm": 2.9740953063582145,
      "learning_rate": 9.101619691101596e-06,
      "loss": 0.6905,
      "step": 850
    },
    {
      "epoch": 1.9666474985623923,
      "grad_norm": 3.6304159606485937,
      "learning_rate": 8.926826905863948e-06,
      "loss": 0.6452,
      "step": 855
    },
    {
      "epoch": 1.9781483611270847,
      "grad_norm": 3.6870581665199453,
      "learning_rate": 8.753014993279568e-06,
      "loss": 0.6116,
      "step": 860
    },
    {
      "epoch": 1.9896492236917769,
      "grad_norm": 2.391327811530145,
      "learning_rate": 8.580212025552108e-06,
      "loss": 0.5666,
      "step": 865
    },
    {
      "epoch": 2.001150086256469,
      "grad_norm": 2.8948885744706856,
      "learning_rate": 8.408445911931443e-06,
      "loss": 0.6081,
      "step": 870
    },
    {
      "epoch": 2.0126509488211615,
      "grad_norm": 1.977460185586279,
      "learning_rate": 8.237744394206089e-06,
      "loss": 0.5382,
      "step": 875
    },
    {
      "epoch": 2.024151811385854,
      "grad_norm": 2.7762480882881433,
      "learning_rate": 8.068135042222658e-06,
      "loss": 0.5869,
      "step": 880
    },
    {
      "epoch": 2.0356526739505463,
      "grad_norm": 2.873630712241601,
      "learning_rate": 7.899645249433057e-06,
      "loss": 0.5745,
      "step": 885
    },
    {
      "epoch": 2.0471535365152387,
      "grad_norm": 3.430627593976143,
      "learning_rate": 7.73230222847021e-06,
      "loss": 0.5832,
      "step": 890
    },
    {
      "epoch": 2.058654399079931,
      "grad_norm": 2.4237656667465766,
      "learning_rate": 7.566133006752966e-06,
      "loss": 0.5642,
      "step": 895
    },
    {
      "epoch": 2.0701552616446235,
      "grad_norm": 2.8504086907474755,
      "learning_rate": 7.40116442212094e-06,
      "loss": 0.5115,
      "step": 900
    },
    {
      "epoch": 2.081656124209316,
      "grad_norm": 3.082050477314924,
      "learning_rate": 7.237423118499931e-06,
      "loss": 0.589,
      "step": 905
    },
    {
      "epoch": 2.093156986774008,
      "grad_norm": 2.6998351175123494,
      "learning_rate": 7.0749355415987005e-06,
      "loss": 0.5072,
      "step": 910
    },
    {
      "epoch": 2.1046578493387003,
      "grad_norm": 4.522522241227402,
      "learning_rate": 6.913727934637769e-06,
      "loss": 0.563,
      "step": 915
    },
    {
      "epoch": 2.1161587119033927,
      "grad_norm": 2.89059514327451,
      "learning_rate": 6.7538263341108415e-06,
      "loss": 0.6086,
      "step": 920
    },
    {
      "epoch": 2.127659574468085,
      "grad_norm": 3.7015658048731743,
      "learning_rate": 6.5952565655797535e-06,
      "loss": 0.5442,
      "step": 925
    },
    {
      "epoch": 2.1391604370327775,
      "grad_norm": 3.489185999449341,
      "learning_rate": 6.438044239503343e-06,
      "loss": 0.6069,
      "step": 930
    },
    {
      "epoch": 2.15066129959747,
      "grad_norm": 3.5473240345510293,
      "learning_rate": 6.282214747101158e-06,
      "loss": 0.6008,
      "step": 935
    },
    {
      "epoch": 2.1621621621621623,
      "grad_norm": 2.2982273297995603,
      "learning_rate": 6.1277932562525515e-06,
      "loss": 0.5914,
      "step": 940
    },
    {
      "epoch": 2.1736630247268547,
      "grad_norm": 1.664287972233148,
      "learning_rate": 5.974804707431814e-06,
      "loss": 0.542,
      "step": 945
    },
    {
      "epoch": 2.185163887291547,
      "grad_norm": 2.4025758375618302,
      "learning_rate": 5.823273809680084e-06,
      "loss": 0.4627,
      "step": 950
    },
    {
      "epoch": 2.196664749856239,
      "grad_norm": 2.94015877629668,
      "learning_rate": 5.673225036614591e-06,
      "loss": 0.562,
      "step": 955
    },
    {
      "epoch": 2.2081656124209315,
      "grad_norm": 2.7615592844519212,
      "learning_rate": 5.524682622475978e-06,
      "loss": 0.6348,
      "step": 960
    },
    {
      "epoch": 2.219666474985624,
      "grad_norm": 3.5755222988258812,
      "learning_rate": 5.377670558214214e-06,
      "loss": 0.5878,
      "step": 965
    },
    {
      "epoch": 2.2311673375503163,
      "grad_norm": 1.6659785400012272,
      "learning_rate": 5.232212587613851e-06,
      "loss": 0.5033,
      "step": 970
    },
    {
      "epoch": 2.2426682001150087,
      "grad_norm": 5.267987515442841,
      "learning_rate": 5.088332203459207e-06,
      "loss": 0.5658,
      "step": 975
    },
    {
      "epoch": 2.254169062679701,
      "grad_norm": 2.4331023241786327,
      "learning_rate": 4.946052643740035e-06,
      "loss": 0.5753,
      "step": 980
    },
    {
      "epoch": 2.2656699252443935,
      "grad_norm": 3.3255580895620604,
      "learning_rate": 4.805396887898397e-06,
      "loss": 0.5632,
      "step": 985
    },
    {
      "epoch": 2.2771707878090854,
      "grad_norm": 2.3985343948003055,
      "learning_rate": 4.666387653117266e-06,
      "loss": 0.5404,
      "step": 990
    },
    {
      "epoch": 2.288671650373778,
      "grad_norm": 3.030582760429119,
      "learning_rate": 4.529047390651514e-06,
      "loss": 0.6135,
      "step": 995
    },
    {
      "epoch": 2.3001725129384702,
      "grad_norm": 3.5016398227375216,
      "learning_rate": 4.393398282201788e-06,
      "loss": 0.5344,
      "step": 1000
    },
    {
      "epoch": 2.3001725129384702,
      "eval_loss": 0.60791015625,
      "eval_runtime": 361.0309,
      "eval_samples_per_second": 1.072,
      "eval_steps_per_second": 0.537,
      "step": 1000
    },
    {
      "epoch": 2.3116733755031627,
      "grad_norm": 2.8438793562503792,
      "learning_rate": 4.259462236332014e-06,
      "loss": 0.5667,
      "step": 1005
    },
    {
      "epoch": 2.323174238067855,
      "grad_norm": 3.7633862345268865,
      "learning_rate": 4.127260884930944e-06,
      "loss": 0.5818,
      "step": 1010
    },
    {
      "epoch": 2.3346751006325475,
      "grad_norm": 3.060587660498981,
      "learning_rate": 3.996815579718376e-06,
      "loss": 0.5139,
      "step": 1015
    },
    {
      "epoch": 2.34617596319724,
      "grad_norm": 2.7718023024580805,
      "learning_rate": 3.868147388796722e-06,
      "loss": 0.5433,
      "step": 1020
    },
    {
      "epoch": 2.3576768257619323,
      "grad_norm": 3.0828028415950435,
      "learning_rate": 3.7412770932482695e-06,
      "loss": 0.5893,
      "step": 1025
    },
    {
      "epoch": 2.3691776883266247,
      "grad_norm": 3.2867180056305156,
      "learning_rate": 3.61622518377886e-06,
      "loss": 0.5496,
      "step": 1030
    },
    {
      "epoch": 2.380678550891317,
      "grad_norm": 2.3821438441300766,
      "learning_rate": 3.4930118574084554e-06,
      "loss": 0.5964,
      "step": 1035
    },
    {
      "epoch": 2.392179413456009,
      "grad_norm": 1.3782864908998658,
      "learning_rate": 3.3716570142091498e-06,
      "loss": 0.5877,
      "step": 1040
    },
    {
      "epoch": 2.4036802760207014,
      "grad_norm": 3.1394401339138467,
      "learning_rate": 3.2521802540910998e-06,
      "loss": 0.5611,
      "step": 1045
    },
    {
      "epoch": 2.415181138585394,
      "grad_norm": 4.01274033323384,
      "learning_rate": 3.134600873636973e-06,
      "loss": 0.6749,
      "step": 1050
    },
    {
      "epoch": 2.4266820011500863,
      "grad_norm": 2.9236185216111443,
      "learning_rate": 3.018937862985392e-06,
      "loss": 0.474,
      "step": 1055
    },
    {
      "epoch": 2.4381828637147787,
      "grad_norm": 3.5184247766186743,
      "learning_rate": 2.905209902763825e-06,
      "loss": 0.582,
      "step": 1060
    },
    {
      "epoch": 2.449683726279471,
      "grad_norm": 2.018133854354603,
      "learning_rate": 2.793435361071509e-06,
      "loss": 0.5656,
      "step": 1065
    },
    {
      "epoch": 2.4611845888441635,
      "grad_norm": 3.4777987060480333,
      "learning_rate": 2.683632290512837e-06,
      "loss": 0.5693,
      "step": 1070
    },
    {
      "epoch": 2.4726854514088554,
      "grad_norm": 2.2496146232407823,
      "learning_rate": 2.5758184252817e-06,
      "loss": 0.5581,
      "step": 1075
    },
    {
      "epoch": 2.484186313973548,
      "grad_norm": 3.1474234714370635,
      "learning_rate": 2.4700111782972405e-06,
      "loss": 0.5416,
      "step": 1080
    },
    {
      "epoch": 2.4956871765382402,
      "grad_norm": 3.2410991295836507,
      "learning_rate": 2.3662276383915177e-06,
      "loss": 0.5379,
      "step": 1085
    },
    {
      "epoch": 2.5071880391029326,
      "grad_norm": 2.482716764262956,
      "learning_rate": 2.2644845675495144e-06,
      "loss": 0.5442,
      "step": 1090
    },
    {
      "epoch": 2.518688901667625,
      "grad_norm": 1.8047259742180928,
      "learning_rate": 2.164798398201896e-06,
      "loss": 0.5412,
      "step": 1095
    },
    {
      "epoch": 2.5301897642323175,
      "grad_norm": 2.685558525309926,
      "learning_rate": 2.0671852305710585e-06,
      "loss": 0.5632,
      "step": 1100
    },
    {
      "epoch": 2.54169062679701,
      "grad_norm": 3.6087671559798133,
      "learning_rate": 1.9716608300707706e-06,
      "loss": 0.5859,
      "step": 1105
    },
    {
      "epoch": 2.5531914893617023,
      "grad_norm": 2.381459966082298,
      "learning_rate": 1.878240624759922e-06,
      "loss": 0.5686,
      "step": 1110
    },
    {
      "epoch": 2.5646923519263947,
      "grad_norm": 4.076339727732688,
      "learning_rate": 1.786939702850766e-06,
      "loss": 0.543,
      "step": 1115
    },
    {
      "epoch": 2.576193214491087,
      "grad_norm": 2.7536095552364803,
      "learning_rate": 1.6977728102720159e-06,
      "loss": 0.5803,
      "step": 1120
    },
    {
      "epoch": 2.587694077055779,
      "grad_norm": 2.225445807512864,
      "learning_rate": 1.6107543482872495e-06,
      "loss": 0.5691,
      "step": 1125
    },
    {
      "epoch": 2.5991949396204714,
      "grad_norm": 2.744069790831864,
      "learning_rate": 1.5258983711689829e-06,
      "loss": 0.5717,
      "step": 1130
    },
    {
      "epoch": 2.610695802185164,
      "grad_norm": 2.4288108072918297,
      "learning_rate": 1.443218583928773e-06,
      "loss": 0.5758,
      "step": 1135
    },
    {
      "epoch": 2.6221966647498562,
      "grad_norm": 2.8650419626114756,
      "learning_rate": 1.3627283401037249e-06,
      "loss": 0.5138,
      "step": 1140
    },
    {
      "epoch": 2.6336975273145486,
      "grad_norm": 4.179261744007206,
      "learning_rate": 1.284440639599785e-06,
      "loss": 0.5963,
      "step": 1145
    },
    {
      "epoch": 2.645198389879241,
      "grad_norm": 3.1427996971837047,
      "learning_rate": 1.208368126592143e-06,
      "loss": 0.5487,
      "step": 1150
    },
    {
      "epoch": 2.6566992524439335,
      "grad_norm": 2.5835656495286803,
      "learning_rate": 1.1345230874830537e-06,
      "loss": 0.5858,
      "step": 1155
    },
    {
      "epoch": 2.6682001150086254,
      "grad_norm": 1.9712839124104287,
      "learning_rate": 1.0629174489175165e-06,
      "loss": 0.5267,
      "step": 1160
    },
    {
      "epoch": 2.679700977573318,
      "grad_norm": 4.1073992234924015,
      "learning_rate": 9.935627758569683e-07,
      "loss": 0.6081,
      "step": 1165
    },
    {
      "epoch": 2.6912018401380102,
      "grad_norm": 4.122954202089338,
      "learning_rate": 9.264702697114736e-07,
      "loss": 0.5511,
      "step": 1170
    },
    {
      "epoch": 2.7027027027027026,
      "grad_norm": 2.4922265927677545,
      "learning_rate": 8.616507665305701e-07,
      "loss": 0.5673,
      "step": 1175
    },
    {
      "epoch": 2.714203565267395,
      "grad_norm": 2.053205188943034,
      "learning_rate": 7.991147352531602e-07,
      "loss": 0.5262,
      "step": 1180
    },
    {
      "epoch": 2.7257044278320874,
      "grad_norm": 2.6449223723631494,
      "learning_rate": 7.388722760166855e-07,
      "loss": 0.5701,
      "step": 1185
    },
    {
      "epoch": 2.73720529039678,
      "grad_norm": 3.533452200672269,
      "learning_rate": 6.809331185258444e-07,
      "loss": 0.5518,
      "step": 1190
    },
    {
      "epoch": 2.7487061529614722,
      "grad_norm": 4.087793556297321,
      "learning_rate": 6.253066204811797e-07,
      "loss": 0.5319,
      "step": 1195
    },
    {
      "epoch": 2.7602070155261647,
      "grad_norm": 2.394380542546084,
      "learning_rate": 5.72001766067709e-07,
      "loss": 0.6043,
      "step": 1200
    },
    {
      "epoch": 2.7602070155261647,
      "eval_loss": 0.60498046875,
      "eval_runtime": 361.3137,
      "eval_samples_per_second": 1.071,
      "eval_steps_per_second": 0.537,
      "step": 1200
    },
    {
      "epoch": 2.771707878090857,
      "grad_norm": 3.4323260495643266,
      "learning_rate": 5.210271645039039e-07,
      "loss": 0.5204,
      "step": 1205
    },
    {
      "epoch": 2.783208740655549,
      "grad_norm": 1.7575618589486002,
      "learning_rate": 4.723910486512184e-07,
      "loss": 0.5196,
      "step": 1210
    },
    {
      "epoch": 2.7947096032202414,
      "grad_norm": 4.022127624645744,
      "learning_rate": 4.2610127368441665e-07,
      "loss": 0.5613,
      "step": 1215
    },
    {
      "epoch": 2.806210465784934,
      "grad_norm": 5.21061506431712,
      "learning_rate": 3.8216531582287884e-07,
      "loss": 0.5211,
      "step": 1220
    },
    {
      "epoch": 2.8177113283496262,
      "grad_norm": 3.029670563026943,
      "learning_rate": 3.405902711231257e-07,
      "loss": 0.5674,
      "step": 1225
    },
    {
      "epoch": 2.8292121909143186,
      "grad_norm": 2.622305225539574,
      "learning_rate": 3.0138285433275514e-07,
      "loss": 0.5293,
      "step": 1230
    },
    {
      "epoch": 2.840713053479011,
      "grad_norm": 4.465816782110454,
      "learning_rate": 2.6454939780592655e-07,
      "loss": 0.5897,
      "step": 1235
    },
    {
      "epoch": 2.852213916043703,
      "grad_norm": 2.6669555895623054,
      "learning_rate": 2.300958504806472e-07,
      "loss": 0.573,
      "step": 1240
    },
    {
      "epoch": 2.8637147786083954,
      "grad_norm": 2.035011579063159,
      "learning_rate": 1.9802777691795558e-07,
      "loss": 0.54,
      "step": 1245
    },
    {
      "epoch": 2.875215641173088,
      "grad_norm": 2.6840197773854655,
      "learning_rate": 1.6835035640319408e-07,
      "loss": 0.5943,
      "step": 1250
    },
    {
      "epoch": 2.88671650373778,
      "grad_norm": 2.7637353318089435,
      "learning_rate": 1.41068382109516e-07,
      "loss": 0.5479,
      "step": 1255
    },
    {
      "epoch": 2.8982173663024726,
      "grad_norm": 2.4028327593112095,
      "learning_rate": 1.1618626032372847e-07,
      "loss": 0.539,
      "step": 1260
    },
    {
      "epoch": 2.909718228867165,
      "grad_norm": 1.981891889902582,
      "learning_rate": 9.370800973465488e-08,
      "loss": 0.602,
      "step": 1265
    },
    {
      "epoch": 2.9212190914318574,
      "grad_norm": 3.433167207311446,
      "learning_rate": 7.363726078406574e-08,
      "loss": 0.529,
      "step": 1270
    },
    {
      "epoch": 2.93271995399655,
      "grad_norm": 2.5620545860365094,
      "learning_rate": 5.5977255080339464e-08,
      "loss": 0.5156,
      "step": 1275
    },
    {
      "epoch": 2.9442208165612422,
      "grad_norm": 2.429238841387908,
      "learning_rate": 4.073084487490941e-08,
      "loss": 0.6073,
      "step": 1280
    },
    {
      "epoch": 2.9557216791259346,
      "grad_norm": 3.089518632114937,
      "learning_rate": 2.7900492601592442e-08,
      "loss": 0.5726,
      "step": 1285
    },
    {
      "epoch": 2.967222541690627,
      "grad_norm": 1.822514861968298,
      "learning_rate": 1.748827047889867e-08,
      "loss": 0.5237,
      "step": 1290
    },
    {
      "epoch": 2.978723404255319,
      "grad_norm": 3.4911974696710275,
      "learning_rate": 9.495860175334215e-09,
      "loss": 0.5737,
      "step": 1295
    },
    {
      "epoch": 2.9902242668200114,
      "grad_norm": 3.089987420610458,
      "learning_rate": 3.924552537808435e-09,
      "loss": 0.5558,
      "step": 1300
    },
    {
      "epoch": 2.9948246118458886,
      "step": 1302,
      "total_flos": 1955992756289536.0,
      "train_loss": 0.6979786020270141,
      "train_runtime": 28949.23,
      "train_samples_per_second": 0.36,
      "train_steps_per_second": 0.045
    }
  ],
  "logging_steps": 5,
  "max_steps": 1302,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "total_flos": 1955992756289536.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
